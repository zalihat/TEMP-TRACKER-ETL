# Use a base image with Python
# FROM python:3.9-slim
FROM python:3.12-slim

# Install Java (Spark requires Java)
# RUN apt-get update && apt-get install -y openjdk-11-jre wget tar
RUN apt-get update && apt-get install -y openjdk-17-jre wget tar


# Download and install Apache Spark
ENV SPARK_VERSION=3.5.4
RUN wget https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz \
    && tar -xzf spark-$SPARK_VERSION-bin-hadoop3.tgz -C /opt \
    && ln -s /opt/spark-$SPARK_VERSION-bin-hadoop3 /opt/spark \
    && rm spark-$SPARK_VERSION-bin-hadoop3.tgz

# Set Spark environment variables
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH
ENV PYSPARK_PYTHON=python3.12
ENV PYSPARK_DRIVER_PYTHON=python3.12

# Install PySpark
RUN pip install pyspark

# Copy your Spark application Python script
COPY app.py /opt/spark/app.py

# Default command to keep the container running
CMD ["tail", "-f", "/dev/null"]