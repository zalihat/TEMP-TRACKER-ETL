# some datalake/db give you option to partition

1. No pyspark for now.
2. Get apache airflow to run the pipeline
3. writting to a database(postgres db)
4. Get pyspark to work


next steps

1. use pyspark
2. change api and transformation will be to create a fact and dimention tables.
3. writting to a database(postgres db) using spark. 