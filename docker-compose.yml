version: "3"

services:
  spark-master:
    build: 
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    environment:
      - PYSPARK_PYTHON=python3.12
      - PYSPARK_DRIVER_PYTHON=python3.12
    hostname: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "0.0.0.0"]
    ports:
      - "7077:7077"  # Spark master port
      - "8080:8080"  # Spark UI
    

  spark-worker-1:
    build: 
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-1
    environment:
      - PYSPARK_PYTHON=python3.12
      - PYSPARK_DRIVER_PYTHON=python3.12
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]

  spark-worker-2:
    build: 
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-2
    environment:
      - PYSPARK_PYTHON=python3.12
      - PYSPARK_DRIVER_PYTHON=python3.12
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
  airflow:
    image: airflow:latest
    volumes:
      - ./airflow:/opt/airflow
    ports:
      - "8090:8080"
    command: airflow standalone
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - 5432:5432
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      retries: 5
    restart: always
  spark-job:
    build: 
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-job
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-submit", "--master", "spark://spark-master:7077", "/opt/spark/app.py"]
volumes:
  postgres_data:
  # airflow:
  #   image: apache/airflow:2.7.0
  #   container_name: airflow
  #   hostname: airflow
  #   restart: always
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #     - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
  #     - AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE=UTC
  #     - AIRFLOW__CORE__FERNET_KEY=<your_fernet_key>  # Generate if needed
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   ports:
  #     - "8089:8080"  # Airflow UI
  #   command: >
  #     bash -c "
  #     airflow db init &&
  #     airflow users create --username airflow --password airflow --firstname Air --lastname Flow --role Admin --email airflow@example.com &&
  #     airflow webserver & airflow scheduler"

 
