[2025-02-04T21:45:16.758+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-04T21:45:16.782+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Temp_elt_postgres.transform_weather_data manual__2025-02-04T21:44:58.039352+00:00 [queued]>
[2025-02-04T21:45:16.791+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Temp_elt_postgres.transform_weather_data manual__2025-02-04T21:44:58.039352+00:00 [queued]>
[2025-02-04T21:45:16.792+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 2
[2025-02-04T21:45:16.838+0000] {taskinstance.py:2889} INFO - Executing <Task(SparkSubmitOperator): transform_weather_data> on 2025-02-04 21:44:58.039352+00:00
[2025-02-04T21:45:16.849+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'Temp_elt_postgres', 'transform_weather_data', 'manual__2025-02-04T21:44:58.039352+00:00', '--job-id', '68', '--raw', '--subdir', 'DAGS_FOLDER/weather_etl_postgres.py', '--cfg-path', '/tmp/tmpy19aom6e']
[2025-02-04T21:45:16.851+0000] {standard_task_runner.py:105} INFO - Job 68: Subtask transform_weather_data
[2025-02-04T21:45:16.852+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=433) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-02-04T21:45:16.854+0000] {standard_task_runner.py:72} INFO - Started process 443 to run task
[2025-02-04T21:45:16.990+0000] {task_command.py:467} INFO - Running <TaskInstance: Temp_elt_postgres.transform_weather_data manual__2025-02-04T21:44:58.039352+00:00 [running]> on host a5e0fb099962
[2025-02-04T21:45:17.146+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='Temp_elt_postgres' AIRFLOW_CTX_TASK_ID='transform_weather_data' AIRFLOW_CTX_EXECUTION_DATE='2025-02-04T21:44:58.039352+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-04T21:44:58.039352+00:00'
[2025-02-04T21:45:17.148+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-02-04T21:45:17.230+0000] {base.py:84} INFO - Retrieving connection 'spark_default'
[2025-02-04T21:45:17.232+0000] {spark_submit.py:474} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --executor-cores 1 --executor-memory 1g --driver-memory 1g --name arrow-spark --verbose --queue root.default --deploy-mode client /opt/airflow/tasks/transform.py
[2025-02-04T21:45:19.157+0000] {spark_submit.py:641} INFO - Using properties file: null
[2025-02-04T21:45:19.318+0000] {spark_submit.py:641} INFO - Parsed arguments:
[2025-02-04T21:45:19.319+0000] {spark_submit.py:641} INFO - master                  spark://spark-master:7077
[2025-02-04T21:45:19.319+0000] {spark_submit.py:641} INFO - remote                  null
[2025-02-04T21:45:19.320+0000] {spark_submit.py:641} INFO - deployMode              client
[2025-02-04T21:45:19.321+0000] {spark_submit.py:641} INFO - executorMemory          1g
[2025-02-04T21:45:19.324+0000] {spark_submit.py:641} INFO - executorCores           1
[2025-02-04T21:45:19.325+0000] {spark_submit.py:641} INFO - totalExecutorCores      null
[2025-02-04T21:45:19.325+0000] {spark_submit.py:641} INFO - propertiesFile          null
[2025-02-04T21:45:19.326+0000] {spark_submit.py:641} INFO - driverMemory            1g
[2025-02-04T21:45:19.331+0000] {spark_submit.py:641} INFO - driverCores             null
[2025-02-04T21:45:19.332+0000] {spark_submit.py:641} INFO - driverExtraClassPath    null
[2025-02-04T21:45:19.332+0000] {spark_submit.py:641} INFO - driverExtraLibraryPath  null
[2025-02-04T21:45:19.333+0000] {spark_submit.py:641} INFO - driverExtraJavaOptions  null
[2025-02-04T21:45:19.336+0000] {spark_submit.py:641} INFO - supervise               false
[2025-02-04T21:45:19.337+0000] {spark_submit.py:641} INFO - queue                   root.default
[2025-02-04T21:45:19.338+0000] {spark_submit.py:641} INFO - numExecutors            null
[2025-02-04T21:45:19.339+0000] {spark_submit.py:641} INFO - files                   null
[2025-02-04T21:45:19.342+0000] {spark_submit.py:641} INFO - pyFiles                 null
[2025-02-04T21:45:19.343+0000] {spark_submit.py:641} INFO - archives                null
[2025-02-04T21:45:19.343+0000] {spark_submit.py:641} INFO - mainClass               null
[2025-02-04T21:45:19.344+0000] {spark_submit.py:641} INFO - primaryResource         file:/opt/airflow/tasks/transform.py
[2025-02-04T21:45:19.344+0000] {spark_submit.py:641} INFO - name                    arrow-spark
[2025-02-04T21:45:19.345+0000] {spark_submit.py:641} INFO - childArgs               []
[2025-02-04T21:45:19.345+0000] {spark_submit.py:641} INFO - jars                    null
[2025-02-04T21:45:19.346+0000] {spark_submit.py:641} INFO - packages                null
[2025-02-04T21:45:19.349+0000] {spark_submit.py:641} INFO - packagesExclusions      null
[2025-02-04T21:45:19.349+0000] {spark_submit.py:641} INFO - repositories            null
[2025-02-04T21:45:19.350+0000] {spark_submit.py:641} INFO - verbose                 true
[2025-02-04T21:45:19.350+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:19.351+0000] {spark_submit.py:641} INFO - Spark properties used, including those specified through
[2025-02-04T21:45:19.351+0000] {spark_submit.py:641} INFO - --conf and those from the properties file null:
[2025-02-04T21:45:19.351+0000] {spark_submit.py:641} INFO - (spark.driver.memory,1g)
[2025-02-04T21:45:19.352+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:19.353+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:19.619+0000] {spark_submit.py:641} INFO - Main class:
[2025-02-04T21:45:19.620+0000] {spark_submit.py:641} INFO - org.apache.spark.deploy.PythonRunner
[2025-02-04T21:45:19.626+0000] {spark_submit.py:641} INFO - Arguments:
[2025-02-04T21:45:19.627+0000] {spark_submit.py:641} INFO - file:/opt/airflow/tasks/transform.py
[2025-02-04T21:45:19.627+0000] {spark_submit.py:641} INFO - null
[2025-02-04T21:45:19.631+0000] {spark_submit.py:641} INFO - Spark config:
[2025-02-04T21:45:19.632+0000] {spark_submit.py:641} INFO - (spark.app.name,arrow-spark)
[2025-02-04T21:45:19.635+0000] {spark_submit.py:641} INFO - (spark.app.submitTime,1738705519593)
[2025-02-04T21:45:19.636+0000] {spark_submit.py:641} INFO - (spark.driver.memory,1g)
[2025-02-04T21:45:19.640+0000] {spark_submit.py:641} INFO - (spark.executor.cores,1)
[2025-02-04T21:45:19.644+0000] {spark_submit.py:641} INFO - (spark.executor.memory,1g)
[2025-02-04T21:45:19.645+0000] {spark_submit.py:641} INFO - (spark.master,spark://spark-master:7077)
[2025-02-04T21:45:19.649+0000] {spark_submit.py:641} INFO - (spark.submit.deployMode,client)
[2025-02-04T21:45:19.650+0000] {spark_submit.py:641} INFO - (spark.submit.pyFiles,)
[2025-02-04T21:45:19.653+0000] {spark_submit.py:641} INFO - Classpath elements:
[2025-02-04T21:45:19.654+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:19.658+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:19.659+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:21.358+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SparkContext: Running Spark version 3.5.4
[2025-02-04T21:45:21.360+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, amd64
[2025-02-04T21:45:21.361+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SparkContext: Java version 17.0.13
[2025-02-04T21:45:21.451+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-02-04T21:45:21.577+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO ResourceUtils: ==============================================================
[2025-02-04T21:45:21.580+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-02-04T21:45:21.581+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO ResourceUtils: ==============================================================
[2025-02-04T21:45:21.582+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SparkContext: Submitted application: WeatherDataProcessing
[2025-02-04T21:45:21.606+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-02-04T21:45:21.619+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
[2025-02-04T21:45:21.624+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-02-04T21:45:21.694+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SecurityManager: Changing view acls to: airflow
[2025-02-04T21:45:21.695+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SecurityManager: Changing modify acls to: airflow
[2025-02-04T21:45:21.696+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SecurityManager: Changing view acls groups to:
[2025-02-04T21:45:21.699+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SecurityManager: Changing modify acls groups to:
[2025-02-04T21:45:21.700+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY
[2025-02-04T21:45:21.993+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:21 INFO Utils: Successfully started service 'sparkDriver' on port 37821.
[2025-02-04T21:45:22.046+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO SparkEnv: Registering MapOutputTracker
[2025-02-04T21:45:22.091+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO SparkEnv: Registering BlockManagerMaster
[2025-02-04T21:45:22.119+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-02-04T21:45:22.120+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-02-04T21:45:22.124+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-02-04T21:45:22.153+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2f8de56f-abe0-4d40-bd2a-1fd92b88e235
[2025-02-04T21:45:22.205+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-02-04T21:45:22.228+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-02-04T21:45:22.401+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-02-04T21:45:22.502+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-02-04T21:45:22.724+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-02-04T21:45:22.781+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 31 ms (0 ms spent in bootstraps)
[2025-02-04T21:45:22.892+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250204214522-0002
[2025-02-04T21:45:22.909+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/0 on worker-20250204212818-172.18.0.6-45979 (172.18.0.6:45979) with 1 core(s)
[2025-02-04T21:45:22.923+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40667.
[2025-02-04T21:45:22.924+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO NettyBlockTransferService: Server created on a5e0fb099962:40667
[2025-02-04T21:45:22.927+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/0 on hostPort 172.18.0.6:45979 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.928+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/1 on worker-20250204212818-172.18.0.6-45979 (172.18.0.6:45979) with 1 core(s)
[2025-02-04T21:45:22.936+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/1 on hostPort 172.18.0.6:45979 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.937+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/2 on worker-20250204212818-172.18.0.6-45979 (172.18.0.6:45979) with 1 core(s)
[2025-02-04T21:45:22.938+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/2 on hostPort 172.18.0.6:45979 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.938+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/3 on worker-20250204212818-172.18.0.6-45979 (172.18.0.6:45979) with 1 core(s)
[2025-02-04T21:45:22.939+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/3 on hostPort 172.18.0.6:45979 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.942+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/4 on worker-20250204212818-172.18.0.6-45979 (172.18.0.6:45979) with 1 core(s)
[2025-02-04T21:45:22.943+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/4 on hostPort 172.18.0.6:45979 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.948+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/5 on worker-20250204212818-172.18.0.6-45979 (172.18.0.6:45979) with 1 core(s)
[2025-02-04T21:45:22.949+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/5 on hostPort 172.18.0.6:45979 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.950+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/6 on worker-20250204212818-172.18.0.5-42327 (172.18.0.5:42327) with 1 core(s)
[2025-02-04T21:45:22.954+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/6 on hostPort 172.18.0.5:42327 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.955+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/7 on worker-20250204212818-172.18.0.5-42327 (172.18.0.5:42327) with 1 core(s)
[2025-02-04T21:45:22.956+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/7 on hostPort 172.18.0.5:42327 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.958+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-02-04T21:45:22.972+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/8 on worker-20250204212818-172.18.0.5-42327 (172.18.0.5:42327) with 1 core(s)
[2025-02-04T21:45:22.981+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/8 on hostPort 172.18.0.5:42327 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.982+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/9 on worker-20250204212818-172.18.0.5-42327 (172.18.0.5:42327) with 1 core(s)
[2025-02-04T21:45:22.984+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/9 on hostPort 172.18.0.5:42327 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.985+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/10 on worker-20250204212818-172.18.0.5-42327 (172.18.0.5:42327) with 1 core(s)
[2025-02-04T21:45:22.985+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/10 on hostPort 172.18.0.5:42327 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.987+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250204214522-0002/11 on worker-20250204212818-172.18.0.5-42327 (172.18.0.5:42327) with 1 core(s)
[2025-02-04T21:45:22.988+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250204214522-0002/11 on hostPort 172.18.0.5:42327 with 1 core(s), 1024.0 MiB RAM
[2025-02-04T21:45:22.988+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a5e0fb099962, 40667, None)
[2025-02-04T21:45:23.027+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO BlockManagerMasterEndpoint: Registering block manager a5e0fb099962:40667 with 434.4 MiB RAM, BlockManagerId(driver, a5e0fb099962, 40667, None)
[2025-02-04T21:45:23.028+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a5e0fb099962, 40667, None)
[2025-02-04T21:45:23.030+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a5e0fb099962, 40667, None)
[2025-02-04T21:45:23.107+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/6 is now RUNNING
[2025-02-04T21:45:23.133+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/0 is now RUNNING
[2025-02-04T21:45:23.179+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/7 is now RUNNING
[2025-02-04T21:45:23.194+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/1 is now RUNNING
[2025-02-04T21:45:23.223+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/10 is now RUNNING
[2025-02-04T21:45:23.259+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/2 is now RUNNING
[2025-02-04T21:45:23.273+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/3 is now RUNNING
[2025-02-04T21:45:23.294+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/9 is now RUNNING
[2025-02-04T21:45:23.445+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/8 is now RUNNING
[2025-02-04T21:45:23.456+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/5 is now RUNNING
[2025-02-04T21:45:23.468+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/4 is now RUNNING
[2025-02-04T21:45:23.475+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250204214522-0002/11 is now RUNNING
[2025-02-04T21:45:23.949+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:23 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-02-04T21:45:24.743+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 06:33.parquet
[2025-02-04T21:45:25.019+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-02-04T21:45:25.049+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:25 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2025-02-04T21:45:28.865+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:28 INFO InMemoryFileIndex: It took 285 ms to list leaf files for 1 paths.
[2025-02-04T21:45:31.338+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:31 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:45:31.435+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:31 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:45:31.437+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:31 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:45:31.443+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:31 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:45:31.460+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:31 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:45:31.473+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:45:32.139+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.4 KiB, free 434.3 MiB)
[2025-02-04T21:45:32.417+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.3 MiB)
[2025-02-04T21:45:32.438+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:32.548+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:45:32.667+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:45:32.673+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-02-04T21:45:45.437+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:45 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:44078) with ID 9,  ResourceProfileId 0
[2025-02-04T21:45:46.202+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:46 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:59830) with ID 5,  ResourceProfileId 0
[2025-02-04T21:45:46.469+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:42729 with 434.4 MiB RAM, BlockManagerId(9, 172.18.0.5, 42729, None)
[2025-02-04T21:45:46.889+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:47.016+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:34233 with 434.4 MiB RAM, BlockManagerId(5, 172.18.0.6, 34233, None)
[2025-02-04T21:45:47.278+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:47 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:44090) with ID 7,  ResourceProfileId 0
[2025-02-04T21:45:47.797+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:47 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:44092) with ID 6,  ResourceProfileId 0
[2025-02-04T21:45:48.157+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:48 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:59876) with ID 1,  ResourceProfileId 0
[2025-02-04T21:45:48.740+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:43159 with 434.4 MiB RAM, BlockManagerId(7, 172.18.0.5, 43159, None)
[2025-02-04T21:45:49.066+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:49.155+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:46693 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.6, 46693, None)
[2025-02-04T21:45:49.505+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:34691 with 434.4 MiB RAM, BlockManagerId(6, 172.18.0.5, 34691, None)
[2025-02-04T21:45:49.559+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:49 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:59892) with ID 2,  ResourceProfileId 0
[2025-02-04T21:45:50.193+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:50 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:37897 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.6, 37897, None)
[2025-02-04T21:45:50.604+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:50 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:46236) with ID 11,  ResourceProfileId 0
[2025-02-04T21:45:50.667+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:50 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:59912) with ID 3,  ResourceProfileId 0
[2025-02-04T21:45:51.144+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:59894) with ID 0,  ResourceProfileId 0
[2025-02-04T21:45:51.171+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:46242) with ID 10,  ResourceProfileId 0
[2025-02-04T21:45:51.214+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:33577 with 434.4 MiB RAM, BlockManagerId(11, 172.18.0.5, 33577, None)
[2025-02-04T21:45:51.269+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:38973 with 434.4 MiB RAM, BlockManagerId(3, 172.18.0.6, 38973, None)
[2025-02-04T21:45:51.474+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:35135 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.6, 35135, None)
[2025-02-04T21:45:51.631+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:37863 with 434.4 MiB RAM, BlockManagerId(10, 172.18.0.5, 37863, None)
[2025-02-04T21:45:51.761+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:33170) with ID 4,  ResourceProfileId 0
[2025-02-04T21:45:51.965+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:51 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:46248) with ID 8,  ResourceProfileId 0
[2025-02-04T21:45:52.019+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:40911 with 434.4 MiB RAM, BlockManagerId(4, 172.18.0.6, 40911, None)
[2025-02-04T21:45:52.115+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:45661 with 434.4 MiB RAM, BlockManagerId(8, 172.18.0.5, 45661, None)
[2025-02-04T21:45:52.343+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:52 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.5 executor 9): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:45:52.344+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:45:52.350+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:45:52.354+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:45:52.354+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:45:52.357+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:45:52.362+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:45:52.364+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:45:52.367+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:45:52.370+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:45:52.374+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:45:52.380+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:45:52.381+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:45:52.386+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:45:52.387+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:45:52.387+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:45:52.388+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:45:52.391+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:45:52.392+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:45:52.393+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:45:52.394+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:45:52.398+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:45:52.399+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:45:52.400+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:45:52.405+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:33.parquet does not exist
[2025-02-04T21:45:52.405+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:45:52.406+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:45:52.407+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:45:52.408+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:45:52.410+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:45:52.411+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:45:52.412+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:45:52.416+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:45:52.416+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:45:52.417+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:45:52.418+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:45:52.423+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:45:52.427+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:45:52.428+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:45:52.430+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:45:52.431+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:45:52.431+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:45:52.434+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:45:52.434+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:45:52.436+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:45:52.439+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:45:52.440+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:45:52.440+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:45:52.441+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:45:52.443+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:45:52.444+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:45:52.446+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:45:52.447+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:52.447+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:52 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:52.792+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:53.696+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:53 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:45:53.701+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:53 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:54.131+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:54.759+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:54 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on 172.18.0.5, executor 10: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:45:54.761+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:54 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:55.076+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:55.732+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on 172.18.0.5, executor 8: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:45:55.733+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
[2025-02-04T21:45:55.736+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-02-04T21:45:55.739+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 INFO TaskSchedulerImpl: Cancelling stage 0
[2025-02-04T21:45:55.740+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.5 executor 8): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:45:55.741+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:45:55.742+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:45:55.742+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:45:55.743+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:45:55.744+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:45:55.745+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:45:55.746+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:45:55.747+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:45:55.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:45:55.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:45:55.749+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:45:55.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:45:55.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:45:55.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:45:55.752+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:45:55.753+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:45:55.754+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:45:55.754+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:45:55.755+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:45:55.756+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:45:55.757+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:45:55.758+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:45:55.759+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:45:55.759+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:33.parquet does not exist
[2025-02-04T21:45:55.760+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:45:55.762+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:45:55.762+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:45:55.763+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:45:55.764+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:45:55.764+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:45:55.765+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:45:55.766+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:45:55.767+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:45:55.767+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:45:55.769+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:45:55.770+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:45:55.770+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:45:55.771+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:45:55.771+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:45:55.772+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:45:55.773+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:45:55.773+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:45:55.774+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:45:55.774+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:45:55.775+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:45:55.776+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:45:55.776+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:45:55.777+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:45:55.778+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:45:55.779+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:45:55.780+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:45:55.781+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:55.782+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:45:55.783+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) failed in 24.015 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.5 executor 8): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:45:55.784+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:45:55.784+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:45:55.785+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:45:55.786+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:45:55.787+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:45:55.788+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:45:55.789+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:45:55.790+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:45:55.790+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:45:55.791+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:45:55.792+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:45:55.792+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:45:55.793+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:45:55.794+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:45:55.795+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:45:55.796+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:45:55.796+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:45:55.797+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:45:55.797+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:45:55.798+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:45:55.799+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:45:55.799+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:45:55.800+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:45:55.800+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:33.parquet does not exist
[2025-02-04T21:45:55.801+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:45:55.801+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:45:55.802+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:45:55.803+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:45:55.804+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:45:55.805+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:45:55.805+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:45:55.806+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:45:55.806+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:45:55.809+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:45:55.810+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:45:55.812+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:45:55.813+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:45:55.814+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:45:55.817+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:45:55.817+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:45:55.818+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:45:55.819+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:45:55.830+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:45:55.830+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:45:55.832+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:45:55.832+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:45:55.834+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:45:55.835+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:45:55.837+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:45:55.837+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:45:55.838+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:45:55.839+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:55.840+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:45:55.841+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:55 INFO DAGScheduler: Job 0 failed: parquet at NativeMethodAccessorImpl.java:0, took 24.434300 s
[2025-02-04T21:45:56.025+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 06:33.parquet. Check logs for details.
[2025-02-04T21:45:56.026+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 06:23.parquet
[2025-02-04T21:45:56.055+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2025-02-04T21:45:56.185+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:45:56.187+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:45:56.189+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:45:56.191+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:45:56.192+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:45:56.206+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:45:56.214+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:56.238+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 103.4 KiB, free 434.3 MiB)
[2025-02-04T21:45:56.257+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.3 MiB)
[2025-02-04T21:45:56.266+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:56.272+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:45:56.318+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:56.323+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:45:56.327+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-02-04T21:45:56.333+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:56.335+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:56.383+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:56.392+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:56.714+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:57.508+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:57 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 4) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:45:57.511+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:45:57.512+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:45:57.515+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:45:57.519+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:45:57.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:45:57.521+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:45:57.523+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:45:57.524+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:45:57.527+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:45:57.530+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:45:57.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:45:57.532+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:45:57.533+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:45:57.534+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:45:57.535+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:45:57.535+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:45:57.536+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:45:57.537+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:45:57.537+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:45:57.538+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:45:57.539+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:45:57.540+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:45:57.541+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:45:57.541+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:23.parquet does not exist
[2025-02-04T21:45:57.542+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:45:57.543+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:45:57.546+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:45:57.547+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:45:57.552+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:45:57.552+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:45:57.553+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:45:57.554+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:45:57.554+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:45:57.555+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:45:57.556+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:45:57.557+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:45:57.558+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:45:57.559+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:45:57.560+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:45:57.561+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:45:57.568+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:45:57.569+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:45:57.570+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:45:57.571+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:45:57.573+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:45:57.574+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:45:57.575+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:45:57.576+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:45:57.577+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:45:57.578+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:45:57.586+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:45:57.587+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:57.588+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:57 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 5) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:57.827+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:58.503+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:58 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 5) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:45:58.507+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:58 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 6) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:58.775+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:59.300+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSetManager: Lost task 0.2 in stage 1.0 (TID 6) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:45:59.302+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 7) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:59.360+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSetManager: Lost task 0.3 in stage 1.0 (TID 7) on 172.18.0.6, executor 1: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:45:59.362+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job
[2025-02-04T21:45:59.364+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-02-04T21:45:59.364+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSchedulerImpl: Cancelling stage 1
[2025-02-04T21:45:59.365+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:45:59.366+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:45:59.367+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:45:59.368+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:45:59.369+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:45:59.370+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:45:59.371+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:45:59.371+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:45:59.372+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:45:59.373+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:45:59.374+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:45:59.374+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:45:59.375+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:45:59.375+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:45:59.376+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:45:59.377+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:45:59.378+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:45:59.378+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:45:59.379+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:45:59.379+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:45:59.380+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:45:59.381+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:45:59.382+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:45:59.383+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:45:59.383+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:23.parquet does not exist
[2025-02-04T21:45:59.384+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:45:59.384+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:45:59.385+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:45:59.385+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:45:59.390+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:45:59.391+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:45:59.392+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:45:59.392+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:45:59.395+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:45:59.396+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:45:59.397+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:45:59.400+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:45:59.401+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:45:59.401+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:45:59.405+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:45:59.406+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:45:59.406+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:45:59.407+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:45:59.410+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:45:59.411+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:45:59.411+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:45:59.411+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:45:59.412+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:45:59.415+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:45:59.416+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:45:59.418+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:45:59.419+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:45:59.419+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:59.420+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:45:59.420+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) failed in 3.174 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:45:59.424+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:45:59.424+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:45:59.425+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:45:59.425+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:45:59.426+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:45:59.426+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:45:59.426+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:45:59.427+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:45:59.428+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:45:59.429+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:45:59.431+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:45:59.431+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:45:59.432+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:45:59.432+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:45:59.433+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:45:59.435+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:45:59.436+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:45:59.436+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:45:59.437+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:45:59.437+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:45:59.438+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:45:59.438+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:45:59.439+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:45:59.439+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:23.parquet does not exist
[2025-02-04T21:45:59.440+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:45:59.440+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:45:59.440+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:45:59.441+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:45:59.441+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:45:59.441+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:45:59.442+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:45:59.442+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:45:59.442+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:45:59.443+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:45:59.443+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:45:59.443+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:45:59.443+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:45:59.444+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:45:59.444+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:45:59.444+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:45:59.444+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:45:59.445+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:45:59.445+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:45:59.445+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:45:59.445+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:45:59.446+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:45:59.446+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:45:59.446+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:45:59.447+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:45:59.447+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:45:59.448+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:45:59.448+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:45:59.448+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:45:59.448+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: Job 1 failed: parquet at NativeMethodAccessorImpl.java:0, took 3.176790 s
[2025-02-04T21:45:59.563+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 06:23.parquet. Check logs for details.
[2025-02-04T21:45:59.564+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 12:04.parquet
[2025-02-04T21:45:59.581+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2025-02-04T21:45:59.634+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:45:59.638+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:45:59.642+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:45:59.645+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:45:59.650+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:45:59.651+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:45:59.652+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:45:59.665+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:45:59.689+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:45:59.692+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:59.693+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:45:59.695+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:45:59.698+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-02-04T21:45:59.701+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:45:59.714+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:59.734+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:59.736+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:59.833+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:45:59.901+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 8) (172.18.0.5 executor 10): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:45:59.903+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:45:59.905+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:45:59.906+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:45:59.906+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:45:59.907+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:45:59.908+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:45:59.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:45:59.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:45:59.911+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:45:59.914+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:45:59.915+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:45:59.916+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:45:59.917+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:45:59.918+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:45:59.919+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:45:59.921+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:45:59.922+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:45:59.923+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:45:59.925+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:45:59.926+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:45:59.927+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:45:59.932+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:45:59.934+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:45:59.968+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:04.parquet does not exist
[2025-02-04T21:45:59.970+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:45:59.974+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:45:59.975+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:45:59.978+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:45:59.981+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:45:59.989+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:45:59.991+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:45:59.993+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:45:59.993+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:45:59.995+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:45:59.996+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:45:59.998+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:00.000+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:00.001+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:00.003+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:00.005+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:00.011+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:00.015+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:00.016+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:00.017+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:00.019+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:00.023+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:00.025+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:00.027+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:00.028+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:00.032+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:00.034+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:00.036+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:00.036+0000] {spark_submit.py:641} INFO - 25/02/04 21:45:59 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 9) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:00.076+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:00.128+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 9) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:00.135+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 10) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:00.167+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 10) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:00.168+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 11) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:00.225+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:00.274+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 11) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:00.275+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job
[2025-02-04T21:46:00.276+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-02-04T21:46:00.278+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSchedulerImpl: Cancelling stage 2
[2025-02-04T21:46:00.278+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:00.279+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:00.280+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:00.282+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:00.283+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:00.283+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:00.284+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:00.285+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:00.286+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:00.288+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:00.288+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:00.289+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:00.290+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:00.291+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:00.291+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:00.292+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:00.293+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:00.293+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:00.294+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:00.294+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:00.295+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:00.296+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:00.296+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:00.297+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:00.298+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:04.parquet does not exist
[2025-02-04T21:46:00.299+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:00.300+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:00.300+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:00.301+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:00.301+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:00.302+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:00.302+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:00.303+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:00.304+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:00.305+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:00.306+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:00.306+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:00.307+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:00.307+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:00.308+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:00.309+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:00.310+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:00.311+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:00.312+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:00.314+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:00.315+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:00.315+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:00.316+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:00.317+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:00.317+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:00.318+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:00.319+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:00.320+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:00.321+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:00.322+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.636 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:00.323+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:00.323+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:00.324+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:00.325+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:00.325+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:00.326+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:00.326+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:00.327+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:00.327+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:00.328+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:00.329+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:00.330+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:00.330+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:00.331+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:00.332+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:00.333+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:00.335+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:00.335+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:00.336+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:00.336+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:00.337+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:00.338+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:00.339+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:00.340+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:04.parquet does not exist
[2025-02-04T21:46:00.340+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:00.342+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:00.343+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:00.343+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:00.344+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:00.345+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:00.345+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:00.346+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:00.347+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:00.347+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:00.348+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:00.349+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:00.349+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:00.350+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:00.350+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:00.351+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:00.351+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:00.352+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:00.352+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:00.353+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:00.353+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:00.353+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:00.354+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:00.355+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:00.355+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:00.356+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:00.356+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:00.357+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:00.357+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:00.358+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: Job 2 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.643497 s
[2025-02-04T21:46:00.551+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 12:04.parquet. Check logs for details.
[2025-02-04T21:46:00.552+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:55.parquet
[2025-02-04T21:46:00.568+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:00.606+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:00.608+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:00.609+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:00.613+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:00.614+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:00.615+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:00.620+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:00.628+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:00.649+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:00.653+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:00.654+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:00.658+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:00.659+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:00.660+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:00.661+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-02-04T21:46:00.662+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:00.665+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 12) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:00.927+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:38973 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:01.514+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:01 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 12) (172.18.0.6 executor 3): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:01.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:01.517+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:01.518+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:01.519+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:01.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:01.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:01.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:01.521+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:01.522+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:01.523+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:01.525+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:01.526+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:01.526+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:01.527+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:01.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:01.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:01.529+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:01.530+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:01.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:01.532+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:01.533+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:01.534+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:01.536+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:01.537+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:55.parquet does not exist
[2025-02-04T21:46:01.539+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:01.539+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:01.541+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:01.541+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:01.546+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:01.547+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:01.549+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:01.549+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:01.550+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:01.551+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:01.552+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:01.552+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:01.553+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:01.554+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:01.555+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:01.556+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:01.560+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:01.563+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:01.564+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:01.565+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:01.566+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:01.567+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:01.568+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:01.569+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:01.570+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:01.571+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:01.572+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:01.573+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:01.576+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:01 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 13) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:01.578+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:01.610+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:01 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 13) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:01.612+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:01 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 14) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:01.916+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:02.594+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 14) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:02.597+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 15) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:02.658+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:02.685+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 15) on 172.18.0.6, executor 1: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:02.686+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job
[2025-02-04T21:46:02.687+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-02-04T21:46:02.688+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO TaskSchedulerImpl: Cancelling stage 3
[2025-02-04T21:46:02.689+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:02.690+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:02.690+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:02.691+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:02.692+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:02.692+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:02.693+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:02.693+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:02.694+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:02.695+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:02.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:02.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:02.698+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:02.699+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:02.700+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:02.700+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:02.701+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:02.701+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:02.702+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:02.703+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:02.703+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:02.704+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:02.705+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:02.705+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:02.706+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:55.parquet does not exist
[2025-02-04T21:46:02.707+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:02.707+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:02.711+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:02.712+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:02.713+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:02.714+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:02.715+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:02.715+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:02.716+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:02.716+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:02.717+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:02.718+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:02.718+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:02.719+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:02.720+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:02.721+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:02.721+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:02.722+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:02.722+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:02.723+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:02.724+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:02.724+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:02.725+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:02.726+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:02.726+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:02.727+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:02.727+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:02.728+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:02.728+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:02.729+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) failed in 2.077 s due to Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:02.730+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:02.731+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:02.731+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:02.732+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:02.732+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:02.733+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:02.733+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:02.734+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:02.734+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:02.735+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:02.735+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:02.736+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:02.736+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:02.737+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:02.737+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:02.738+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:02.739+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:02.739+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:02.739+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:02.740+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:02.741+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:02.742+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:02.742+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:02.743+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:55.parquet does not exist
[2025-02-04T21:46:02.743+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:02.743+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:02.744+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:02.744+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:02.745+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:02.745+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:02.746+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:02.747+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:02.747+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:02.748+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:02.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:02.749+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:02.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:02.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:02.751+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:02.751+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:02.751+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:02.752+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:02.752+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:02.753+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:02.753+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:02.754+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:02.754+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:02.755+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:02.756+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:02.756+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:02.757+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:02.757+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:02.757+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:02.758+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO DAGScheduler: Job 3 failed: parquet at NativeMethodAccessorImpl.java:0, took 2.082715 s
[2025-02-04T21:46:02.948+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:55.parquet. Check logs for details.
[2025-02-04T21:46:02.949+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:45.parquet
[2025-02-04T21:46:02.969+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:03.016+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:03.021+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:03.022+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:03.024+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:03.025+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:03.026+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:03.036+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:03.050+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:03.067+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:03.073+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.079+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:03.088+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:03.091+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.099+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-02-04T21:46:03.169+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:03.208+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.210+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.6:38973 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.215+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.258+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.320+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 16) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:03.326+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:03.329+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:03.330+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:03.344+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:03.347+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:03.350+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:03.353+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:03.355+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:03.357+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:03.359+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:03.370+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:03.417+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:03.426+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:03.428+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:03.433+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:03.435+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:03.436+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:03.438+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:03.441+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:03.442+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:03.443+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:03.444+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:03.447+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:03.448+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:45.parquet does not exist
[2025-02-04T21:46:03.450+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:03.451+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:03.452+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:03.453+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:03.455+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:03.456+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:03.457+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:03.458+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:03.460+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:03.462+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:03.464+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:03.466+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:03.469+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:03.470+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:03.472+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:03.473+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:03.475+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:03.477+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:03.478+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:03.479+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:03.481+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:03.482+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:03.497+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:03.502+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:03.504+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:03.506+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:03.509+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:03.510+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:03.512+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSetManager: Starting task 0.1 in stage 4.0 (TID 17) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:03.513+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.514+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSetManager: Lost task 0.1 in stage 4.0 (TID 17) on 172.18.0.6, executor 1: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:03.515+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSetManager: Starting task 0.2 in stage 4.0 (TID 18) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:03.548+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:03.611+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSetManager: Lost task 0.2 in stage 4.0 (TID 18) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:03.618+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSetManager: Starting task 0.3 in stage 4.0 (TID 19) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:03.658+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSetManager: Lost task 0.3 in stage 4.0 (TID 19) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:03.663+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 ERROR TaskSetManager: Task 0 in stage 4.0 failed 4 times; aborting job
[2025-02-04T21:46:03.665+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-02-04T21:46:03.666+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSchedulerImpl: Cancelling stage 4
[2025-02-04T21:46:03.668+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19) (172.18.0.5 executor 9): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:03.673+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:03.680+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:03.683+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:03.684+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:03.686+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:03.689+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:03.690+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:03.691+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:03.693+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:03.694+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:03.697+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:03.707+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:03.708+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:03.709+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:03.710+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:03.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:03.714+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:03.716+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:03.717+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:03.718+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:03.718+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:03.719+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:03.720+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:03.721+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:45.parquet does not exist
[2025-02-04T21:46:03.722+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:03.723+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:03.724+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:03.724+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:03.725+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:03.725+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:03.726+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:03.726+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:03.727+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:03.728+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:03.728+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:03.729+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:03.730+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:03.731+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:03.732+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:03.733+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:03.734+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:03.735+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:03.736+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:03.737+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:03.738+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:03.739+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:03.740+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:03.741+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:03.741+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:03.742+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:03.743+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:03.744+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:03.745+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:03.745+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.636 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19) (172.18.0.5 executor 9): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:03.746+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:03.747+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:03.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:03.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:03.749+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:03.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:03.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:03.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:03.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:03.752+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:03.752+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:03.753+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:03.753+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:03.754+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:03.755+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:03.755+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:03.756+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:03.757+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:03.758+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:03.759+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:03.759+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:03.760+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:03.760+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:03.761+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:45.parquet does not exist
[2025-02-04T21:46:03.761+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:03.762+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:03.763+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:03.763+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:03.764+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:03.765+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:03.765+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:03.765+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:03.766+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:03.766+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:03.767+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:03.767+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:03.767+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:03.768+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:03.768+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:03.769+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:03.769+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:03.770+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:03.771+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:03.771+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:03.772+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:03.773+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:03.773+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:03.774+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:03.775+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:03.775+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:03.776+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:03.777+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:03.777+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:03.778+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO DAGScheduler: Job 4 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.644232 s
[2025-02-04T21:46:03.912+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:45.parquet. Check logs for details.
[2025-02-04T21:46:03.912+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:27.parquet
[2025-02-04T21:46:03.926+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:04.036+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:04.039+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:04.043+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:04.044+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:04.045+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:04.047+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:04.054+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:04.064+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:04.081+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:04.085+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:04.089+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:04.090+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-02-04T21:46:04.094+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:04.095+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 20) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:04.100+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:04.102+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:04.128+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:04.153+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:04.183+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 20) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:04.186+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:04.188+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:04.189+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:04.189+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:04.190+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:04.191+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:04.192+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:04.193+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:04.193+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:04.194+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:04.195+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:04.196+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:04.197+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:04.198+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:04.198+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:04.199+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:04.200+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:04.200+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:04.202+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:04.202+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:04.203+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:04.204+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:04.205+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:04.206+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:27.parquet does not exist
[2025-02-04T21:46:04.206+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:04.207+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:04.211+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:04.211+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:04.214+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:04.215+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:04.215+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:04.217+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:04.218+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:04.219+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:04.221+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:04.222+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:04.224+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:04.227+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:04.228+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:04.230+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:04.231+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:04.233+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:04.234+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:04.235+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:04.236+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:04.237+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:04.238+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:04.239+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:04.240+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:04.241+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:04.241+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:04.242+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:04.243+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 21) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:04.244+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:04.288+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 21) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:04.290+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 22) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:04.332+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:04.398+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 22) on 172.18.0.5, executor 10: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:04.400+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 23) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:04.785+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:05.715+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:05 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 23) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:05.715+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:05 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job
[2025-02-04T21:46:05.718+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:05 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-02-04T21:46:05.719+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:05 INFO TaskSchedulerImpl: Cancelling stage 5
[2025-02-04T21:46:05.723+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 23) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:05.724+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:05.725+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:05.727+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:05.728+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:05.732+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:05.735+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:05.741+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:05.742+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:05.743+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:05.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:05.752+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:05.757+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:05.791+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:05.792+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:05.793+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:05.794+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:05.795+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:05.797+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:05.798+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:05.799+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:05.800+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:05.800+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:05.801+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:05.802+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:27.parquet does not exist
[2025-02-04T21:46:05.806+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:05.807+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:05.808+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:05.809+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:05.810+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:05.811+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:05.815+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:05.817+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:05.827+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:05.832+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:05.851+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:05.852+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:05.865+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:05.866+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:05.871+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:05.875+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:05.877+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:05.879+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:05.881+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:05.882+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:05.883+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:05.883+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:05.884+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:05.887+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:05.888+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:05.889+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:05.890+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:05.891+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:05.892+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:05.893+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:05 INFO DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) failed in 1.673 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 23) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:05.893+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:05.894+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:05.895+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:05.896+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:05.897+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:05.898+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:05.899+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:05.900+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:05.901+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:05.902+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:05.904+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:05.905+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:05.906+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:05.907+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:05.909+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:05.909+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:05.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:05.911+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:05.912+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:05.913+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:05.914+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:05.915+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:05.915+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:05.916+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:27.parquet does not exist
[2025-02-04T21:46:05.917+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:05.919+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:05.921+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:05.925+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:05.926+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:05.928+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:05.929+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:05.930+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:05.931+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:05.932+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:05.933+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:05.934+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:05.935+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:05.936+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:05.939+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:05.940+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:05.941+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:05.943+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:05.945+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:05.946+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:05.947+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:05.949+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:05.950+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:05.950+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:05.951+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:05.952+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:05.953+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:05.954+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:05.956+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:05.957+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:05 INFO DAGScheduler: Job 5 failed: parquet at NativeMethodAccessorImpl.java:0, took 1.688777 s
[2025-02-04T21:46:06.072+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:27.parquet. Check logs for details.
[2025-02-04T21:46:06.073+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:36.parquet
[2025-02-04T21:46:06.098+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2025-02-04T21:46:06.146+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:06.148+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:06.149+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:06.150+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:06.150+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:06.151+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:06.161+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:06.169+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:06.171+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:06.183+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:06.185+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:06.188+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-02-04T21:46:06.189+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:06.196+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:06.198+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 24) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:06.200+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:06.201+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:06.214+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:06.280+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:06.429+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 24) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:06.431+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:06.442+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:06.511+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:06.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:06.523+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:06.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:06.534+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:06.535+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:06.538+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:06.549+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:06.555+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:06.557+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:06.559+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:06.560+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:06.561+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:06.563+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:06.565+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:06.566+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:06.567+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:06.568+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:06.568+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:06.571+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:06.572+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:06.573+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:36.parquet does not exist
[2025-02-04T21:46:06.574+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:06.575+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:06.575+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:06.576+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:06.577+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:06.577+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:06.578+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:06.579+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:06.580+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:06.581+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:06.582+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:06.583+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:06.583+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:06.584+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:06.585+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:06.586+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:06.587+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:06.588+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:06.590+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:06.591+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:06.592+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:06.592+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:06.634+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:06.640+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:06.641+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:06.643+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:06.645+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:06.648+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:06.649+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:06 INFO TaskSetManager: Starting task 0.1 in stage 6.0 (TID 25) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:07.279+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:08.350+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSetManager: Lost task 0.1 in stage 6.0 (TID 25) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:08.351+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSetManager: Starting task 0.2 in stage 6.0 (TID 26) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:08.479+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:08.634+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSetManager: Lost task 0.2 in stage 6.0 (TID 26) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:08.637+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSetManager: Starting task 0.3 in stage 6.0 (TID 27) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:08.755+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSetManager: Lost task 0.3 in stage 6.0 (TID 27) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:08.760+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 ERROR TaskSetManager: Task 0 in stage 6.0 failed 4 times; aborting job
[2025-02-04T21:46:08.761+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-02-04T21:46:08.763+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSchedulerImpl: Cancelling stage 6
[2025-02-04T21:46:08.764+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 27) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:08.766+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:08.767+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:08.768+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:08.769+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:08.770+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:08.771+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:08.772+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:08.772+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:08.773+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:08.777+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:08.779+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:08.781+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:08.782+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:08.783+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:08.785+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:08.786+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:08.788+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:08.789+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:08.790+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:08.791+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:08.792+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:08.793+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:08.795+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:08.796+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:36.parquet does not exist
[2025-02-04T21:46:08.798+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:08.799+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:08.800+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:08.814+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:08.819+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:08.820+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:08.821+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:08.822+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:08.823+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:08.824+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:08.826+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:08.827+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:08.830+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:08.830+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:08.831+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:08.832+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:08.833+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:08.834+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:08.834+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:08.835+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:08.836+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:08.837+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:08.838+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:08.839+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:08.839+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:08.840+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:08.842+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:08.843+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:08.844+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:08.845+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) failed in 2.606 s due to Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 27) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:08.846+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:08.848+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:08.849+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:08.850+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:08.851+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:08.852+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:08.853+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:08.853+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:08.854+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:08.855+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:08.855+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:08.856+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:08.857+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:08.857+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:08.858+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:08.859+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:08.860+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:08.861+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:08.861+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:08.862+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:08.862+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:08.863+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:08.864+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:08.864+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:36.parquet does not exist
[2025-02-04T21:46:08.865+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:08.866+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:08.867+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:08.868+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:08.869+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:08.871+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:08.871+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:08.872+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:08.873+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:08.873+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:08.874+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:08.874+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:08.875+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:08.876+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:08.876+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:08.879+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:08.880+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:08.880+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:08.882+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:08.883+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:08.883+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:08.885+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:08.886+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:08.887+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:08.888+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:08.888+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:08.889+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:08.890+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:08.891+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:08.891+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:08 INFO DAGScheduler: Job 6 failed: parquet at NativeMethodAccessorImpl.java:0, took 2.610511 s
[2025-02-04T21:46:09.005+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:36.parquet. Check logs for details.
[2025-02-04T21:46:09.007+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 05:08.parquet
[2025-02-04T21:46:09.026+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:09.067+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:09.069+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO DAGScheduler: Got job 7 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:09.071+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:09.072+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:09.073+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:09.073+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:09.083+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:09.094+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:09.097+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:09.110+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:09.113+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:09.115+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-02-04T21:46:09.116+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:09.127+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:09.129+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:09.135+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:09.139+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 28) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:09.212+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:09.248+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 28) (172.18.0.5 executor 10): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:09.250+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:09.256+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:09.258+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:09.260+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:09.261+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:09.263+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:09.267+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:09.271+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:09.274+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:09.275+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:09.284+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:09.286+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:09.287+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:09.287+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:09.289+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:09.290+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:09.291+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:09.293+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:09.294+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:09.295+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:09.296+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:09.297+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:09.300+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:09.300+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 05:08.parquet does not exist
[2025-02-04T21:46:09.301+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:09.302+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:09.305+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:09.306+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:09.307+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:09.308+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:09.309+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:09.311+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:09.313+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:09.314+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:09.315+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:09.316+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:09.317+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:09.320+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:09.321+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:09.322+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:09.323+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:09.323+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:09.324+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:09.325+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:09.325+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:09.326+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:09.327+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:09.328+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:09.330+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:09.333+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:09.334+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:09.336+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:09.337+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO TaskSetManager: Starting task 0.1 in stage 7.0 (TID 29) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:09.577+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:10.472+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSetManager: Lost task 0.1 in stage 7.0 (TID 29) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:10.476+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSetManager: Starting task 0.2 in stage 7.0 (TID 30) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:10.518+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:10.587+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSetManager: Lost task 0.2 in stage 7.0 (TID 30) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:10.589+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSetManager: Starting task 0.3 in stage 7.0 (TID 31) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:10.665+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSetManager: Lost task 0.3 in stage 7.0 (TID 31) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:10.669+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 ERROR TaskSetManager: Task 0 in stage 7.0 failed 4 times; aborting job
[2025-02-04T21:46:10.670+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-02-04T21:46:10.673+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSchedulerImpl: Cancelling stage 7
[2025-02-04T21:46:10.674+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 31) (172.18.0.5 executor 7): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:10.675+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:10.676+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:10.677+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:10.679+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:10.681+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:10.682+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:10.683+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:10.686+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:10.689+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:10.690+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:10.691+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:10.691+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:10.693+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:10.694+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:10.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:10.697+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:10.699+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:10.700+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:10.701+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:10.702+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:10.705+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:10.706+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:10.707+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:10.707+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 05:08.parquet does not exist
[2025-02-04T21:46:10.708+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:10.711+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:10.712+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:10.714+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:10.715+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:10.715+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:10.716+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:10.717+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:10.718+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:10.718+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:10.720+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:10.722+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:10.723+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:10.723+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:10.725+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:10.727+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:10.728+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:10.729+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:10.729+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:10.730+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:10.733+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:10.734+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:10.734+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:10.735+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:10.736+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:10.738+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:10.738+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:10.740+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:10.741+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:10.742+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO DAGScheduler: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) failed in 1.595 s due to Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 31) (172.18.0.5 executor 7): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:10.743+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:10.743+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:10.744+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:10.745+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:10.746+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:10.746+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:10.746+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:10.747+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:10.747+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:10.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:10.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:10.748+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:10.749+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:10.749+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:10.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:10.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:10.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:10.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:10.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:10.752+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:10.753+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:10.755+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:10.756+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:10.758+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 05:08.parquet does not exist
[2025-02-04T21:46:10.759+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:10.760+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:10.761+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:10.763+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:10.764+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:10.764+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:10.765+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:10.768+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:10.770+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:10.771+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:10.771+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:10.772+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:10.772+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:10.773+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:10.773+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:10.774+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:10.775+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:10.776+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:10.776+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:10.778+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:10.779+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:10.779+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:10.780+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:10.781+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:10.781+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:10.782+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:10.783+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:10.784+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:10.784+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:10.785+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:10 INFO DAGScheduler: Job 7 failed: parquet at NativeMethodAccessorImpl.java:0, took 1.600908 s
[2025-02-04T21:46:11.015+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 05:08.parquet. Check logs for details.
[2025-02-04T21:46:11.017+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:24.parquet
[2025-02-04T21:46:11.042+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:11.083+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:11.085+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Got job 8 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:11.086+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:11.087+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:11.088+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:11.089+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:11.099+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:11.109+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:11.114+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:11.117+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.120+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:11.122+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:11.124+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-02-04T21:46:11.127+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.139+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 32) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:11.143+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.152+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.188+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.215+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 32) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:11.216+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:11.217+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:11.219+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:11.220+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:11.221+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:11.222+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:11.222+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:11.223+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:11.223+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:11.224+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:11.224+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:11.225+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:11.226+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:11.227+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:11.228+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:11.229+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:11.229+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:11.230+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:11.231+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:11.232+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:11.233+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:11.233+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:11.234+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:11.236+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:24.parquet does not exist
[2025-02-04T21:46:11.239+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:11.240+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:11.241+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:11.247+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:11.249+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:11.253+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:11.254+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:11.255+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:11.258+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:11.259+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:11.269+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:11.271+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:11.276+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:11.280+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:11.287+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:11.290+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:11.292+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:11.296+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:11.297+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:11.300+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:11.303+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:11.308+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:11.319+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:11.330+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:11.386+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:11.387+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:11.388+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:11.388+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:11.390+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSetManager: Starting task 0.1 in stage 8.0 (TID 33) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:11.390+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.406+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSetManager: Lost task 0.1 in stage 8.0 (TID 33) on 172.18.0.5, executor 8: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:11.407+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSetManager: Starting task 0.2 in stage 8.0 (TID 34) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:11.479+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.528+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSetManager: Lost task 0.2 in stage 8.0 (TID 34) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:11.529+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSetManager: Starting task 0.3 in stage 8.0 (TID 35) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:11.599+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.626+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSetManager: Lost task 0.3 in stage 8.0 (TID 35) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:11.628+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 ERROR TaskSetManager: Task 0 in stage 8.0 failed 4 times; aborting job
[2025-02-04T21:46:11.629+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-02-04T21:46:11.630+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSchedulerImpl: Cancelling stage 8
[2025-02-04T21:46:11.631+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 35) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:11.634+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:11.635+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:11.637+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:11.638+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:11.639+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:11.641+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:11.642+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:11.643+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:11.644+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:11.648+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:11.648+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:11.649+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:11.649+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:11.650+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:11.650+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:11.651+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:11.654+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:11.654+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:11.655+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:11.658+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:11.659+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:11.660+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:11.661+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:11.662+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:24.parquet does not exist
[2025-02-04T21:46:11.663+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:11.664+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:11.664+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:11.665+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:11.668+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:11.669+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:11.670+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:11.670+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:11.670+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:11.671+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:11.672+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:11.672+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:11.673+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:11.674+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:11.675+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:11.676+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:11.677+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:11.678+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:11.679+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:11.680+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:11.681+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:11.682+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:11.683+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:11.683+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:11.685+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:11.686+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:11.687+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:11.688+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:11.688+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:11.690+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.539 s due to Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 35) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:11.690+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:11.691+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:11.692+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:11.693+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:11.694+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:11.695+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:11.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:11.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:11.697+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:11.698+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:11.698+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:11.699+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:11.700+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:11.700+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:11.701+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:11.702+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:11.703+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:11.703+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:11.704+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:11.704+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:11.705+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:11.705+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:11.706+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:11.706+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:24.parquet does not exist
[2025-02-04T21:46:11.707+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:11.707+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:11.707+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:11.708+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:11.708+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:11.709+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:11.709+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:11.710+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:11.712+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:11.712+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:11.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:11.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:11.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:11.714+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:11.714+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:11.715+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:11.715+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:11.715+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:11.716+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:11.716+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:11.716+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:11.717+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:11.718+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:11.720+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:11.720+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:11.721+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:11.722+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:11.722+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:11.723+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:11.723+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Job 8 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.545201 s
[2025-02-04T21:46:11.894+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:24.parquet. Check logs for details.
[2025-02-04T21:46:11.895+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:56.parquet
[2025-02-04T21:46:11.906+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:11.941+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:11.944+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Got job 9 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:11.945+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:11.947+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:11.948+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:11.949+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[19] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:11.960+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:11.977+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:11.991+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:11.992+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Removed broadcast_8_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:11.999+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:12.004+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:11 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.005+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:12.007+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-02-04T21:46:12.010+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.013+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 36) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:12.021+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.022+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.072+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.110+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 36) (172.18.0.5 executor 7): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:12.113+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:12.114+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:12.115+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:12.117+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:12.118+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:12.120+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:12.121+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:12.125+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:12.129+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:12.130+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:12.132+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:12.134+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:12.136+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:12.137+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:12.138+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:12.139+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:12.140+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:12.141+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:12.142+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:12.143+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:12.144+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:12.146+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:12.149+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:12.150+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:56.parquet does not exist
[2025-02-04T21:46:12.152+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:12.153+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:12.154+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:12.155+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:12.155+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:12.156+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:12.157+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:12.158+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:12.161+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:12.162+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:12.163+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:12.164+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:12.165+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:12.166+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:12.167+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:12.168+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:12.174+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:12.176+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:12.177+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:12.179+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:12.180+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:12.181+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:12.182+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:12.203+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:12.215+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:12.217+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:12.223+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:12.225+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:12.233+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Starting task 0.1 in stage 9.0 (TID 37) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:12.235+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.241+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Lost task 0.1 in stage 9.0 (TID 37) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:12.243+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Starting task 0.2 in stage 9.0 (TID 38) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:12.363+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.411+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Lost task 0.2 in stage 9.0 (TID 38) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:12.412+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Starting task 0.3 in stage 9.0 (TID 39) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:12.499+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.520+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Lost task 0.3 in stage 9.0 (TID 39) on 172.18.0.5, executor 10: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:12.521+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 ERROR TaskSetManager: Task 0 in stage 9.0 failed 4 times; aborting job
[2025-02-04T21:46:12.523+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-02-04T21:46:12.524+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSchedulerImpl: Cancelling stage 9
[2025-02-04T21:46:12.524+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 39) (172.18.0.5 executor 10): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:12.525+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:12.525+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:12.526+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:12.527+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:12.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:12.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:12.529+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:12.530+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:12.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:12.532+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:12.533+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:12.534+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:12.535+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:12.536+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:12.537+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:12.538+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:12.539+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:12.540+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:12.540+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:12.541+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:12.542+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:12.543+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:12.544+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:12.545+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:56.parquet does not exist
[2025-02-04T21:46:12.546+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:12.546+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:12.547+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:12.547+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:12.548+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:12.548+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:12.549+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:12.549+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:12.550+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:12.551+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:12.551+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:12.552+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:12.553+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:12.554+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:12.555+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:12.555+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:12.556+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:12.556+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:12.557+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:12.558+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:12.558+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:12.559+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:12.559+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:12.560+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:12.561+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:12.561+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:12.562+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:12.562+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:12.563+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:12.563+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.574 s due to Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 39) (172.18.0.5 executor 10): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:12.564+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:12.564+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:12.565+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:12.565+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:12.566+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:12.566+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:12.567+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:12.567+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:12.568+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:12.569+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:12.570+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:12.570+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:12.571+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:12.574+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:12.575+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:12.575+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:12.576+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:12.577+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:12.578+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:12.579+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:12.580+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:12.581+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:12.581+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:12.582+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:56.parquet does not exist
[2025-02-04T21:46:12.583+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:12.583+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:12.586+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:12.587+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:12.588+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:12.589+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:12.590+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:12.592+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:12.593+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:12.593+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:12.595+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:12.595+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:12.596+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:12.597+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:12.598+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:12.598+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:12.600+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:12.600+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:12.600+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:12.601+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:12.602+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:12.603+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:12.603+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:12.604+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:12.605+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:12.606+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:12.607+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:12.607+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:12.608+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:12.609+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Job 9 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.580973 s
[2025-02-04T21:46:12.747+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:56.parquet. Check logs for details.
[2025-02-04T21:46:12.748+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 06:28.parquet
[2025-02-04T21:46:12.765+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:12.804+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:12.805+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Got job 10 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:12.806+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:12.806+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:12.807+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:12.807+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[21] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:12.815+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:12.824+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:12.830+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:12.839+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.842+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.846+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:12.848+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.849+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.850+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:12.853+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-02-04T21:46:12.854+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 40) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:12.863+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.889+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:12.912+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 40) (172.18.0.5 executor 10): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:12.913+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:12.914+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:12.915+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:12.916+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:12.916+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:12.919+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:12.920+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:12.920+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:12.921+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:12.922+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:12.923+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:12.923+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:12.924+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:12.924+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:12.925+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:12.925+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:12.925+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:12.926+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:12.927+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:12.927+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:12.928+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:12.929+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:12.929+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:12.930+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:28.parquet does not exist
[2025-02-04T21:46:12.931+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:12.932+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:12.933+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:12.935+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:12.936+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:12.937+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:12.939+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:12.939+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:12.940+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:12.941+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:12.942+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:12.943+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:12.944+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:12.945+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:12.946+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:12.946+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:12.947+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:12.947+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:12.948+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:12.949+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:12.949+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:12.950+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:12.952+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:12.954+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:12.955+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:12.955+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:12.956+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:12.957+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:12.958+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO TaskSetManager: Starting task 0.1 in stage 10.0 (TID 41) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:12.977+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.6:38973 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.021+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Lost task 0.1 in stage 10.0 (TID 41) on 172.18.0.6, executor 3: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:13.024+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Starting task 0.2 in stage 10.0 (TID 42) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:13.104+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.162+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Lost task 0.2 in stage 10.0 (TID 42) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:13.163+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Starting task 0.3 in stage 10.0 (TID 43) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:13.193+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.234+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Lost task 0.3 in stage 10.0 (TID 43) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:13.239+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 ERROR TaskSetManager: Task 0 in stage 10.0 failed 4 times; aborting job
[2025-02-04T21:46:13.240+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-02-04T21:46:13.251+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSchedulerImpl: Cancelling stage 10
[2025-02-04T21:46:13.254+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 43) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:13.254+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:13.255+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:13.256+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:13.257+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:13.258+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:13.261+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:13.270+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:13.271+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:13.272+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:13.273+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:13.274+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:13.276+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:13.277+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:13.278+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:13.279+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:13.281+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:13.282+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:13.285+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:13.288+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:13.289+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:13.291+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:13.293+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:13.294+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:13.295+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:28.parquet does not exist
[2025-02-04T21:46:13.297+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:13.298+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:13.299+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:13.301+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:13.302+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:13.304+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:13.305+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:13.306+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:13.308+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:13.312+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:13.314+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:13.315+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:13.317+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:13.319+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:13.321+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:13.322+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:13.325+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:13.326+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:13.329+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:13.330+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:13.335+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:13.336+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:13.336+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:13.337+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:13.338+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:13.341+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:13.342+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:13.342+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:13.344+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:13.345+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: ResultStage 10 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.431 s due to Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 43) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:13.346+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:13.346+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:13.347+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:13.349+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:13.350+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:13.351+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:13.352+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:13.354+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:13.354+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:13.355+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:13.356+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:13.358+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:13.361+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:13.362+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:13.363+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:13.364+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:13.364+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:13.365+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:13.366+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:13.367+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:13.367+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:13.369+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:13.370+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:13.370+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:28.parquet does not exist
[2025-02-04T21:46:13.371+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:13.372+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:13.372+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:13.373+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:13.374+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:13.376+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:13.378+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:13.381+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:13.383+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:13.392+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:13.394+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:13.395+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:13.396+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:13.397+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:13.414+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:13.415+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:13.423+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:13.429+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:13.431+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:13.433+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:13.434+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:13.437+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:13.438+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:13.440+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:13.441+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:13.442+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:13.444+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:13.445+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:13.446+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:13.446+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Job 10 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.438363 s
[2025-02-04T21:46:13.513+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 06:28.parquet. Check logs for details.
[2025-02-04T21:46:13.514+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 06:21.parquet
[2025-02-04T21:46:13.528+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:13.559+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:13.561+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Got job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:13.561+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:13.562+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:13.563+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:13.564+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:13.569+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:13.579+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:13.587+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:13.587+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.588+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:13.590+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:13.592+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.6:38973 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.595+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-02-04T21:46:13.598+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 44) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:13.599+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.600+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.605+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.635+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.667+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 44) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:13.669+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:13.671+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:13.672+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:13.673+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:13.674+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:13.675+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:13.679+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:13.681+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:13.682+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:13.683+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:13.685+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:13.686+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:13.687+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:13.688+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:13.689+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:13.690+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:13.692+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:13.693+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:13.694+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:13.695+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:13.696+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:13.696+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:13.697+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:13.698+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:21.parquet does not exist
[2025-02-04T21:46:13.699+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:13.699+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:13.700+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:13.703+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:13.704+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:13.705+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:13.706+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:13.707+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:13.708+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:13.709+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:13.710+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:13.711+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:13.712+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:13.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:13.713+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:13.714+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:13.715+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:13.715+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:13.716+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:13.717+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:13.718+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:13.720+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:13.721+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:13.722+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:13.723+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:13.724+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:13.728+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:13.729+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:13.730+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Starting task 0.1 in stage 11.0 (TID 45) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:13.732+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.741+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Lost task 0.1 in stage 11.0 (TID 45) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:13.746+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Starting task 0.2 in stage 11.0 (TID 46) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:13.809+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.842+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Lost task 0.2 in stage 11.0 (TID 46) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:13.845+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Starting task 0.3 in stage 11.0 (TID 47) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:13.910+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:13.947+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSetManager: Lost task 0.3 in stage 11.0 (TID 47) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:13.948+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 ERROR TaskSetManager: Task 0 in stage 11.0 failed 4 times; aborting job
[2025-02-04T21:46:13.949+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-02-04T21:46:13.952+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSchedulerImpl: Cancelling stage 11
[2025-02-04T21:46:13.953+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 47) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:13.954+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:13.955+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:13.955+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:13.956+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:13.958+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:13.959+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:13.960+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:13.962+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:13.965+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:13.966+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:13.966+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:13.967+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:13.968+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:13.971+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:13.972+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:13.973+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:13.974+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:13.975+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:13.975+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:13.976+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:13.977+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:13.978+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:13.979+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:13.980+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:21.parquet does not exist
[2025-02-04T21:46:13.981+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:13.982+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:13.984+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:13.985+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:13.986+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:13.987+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:13.988+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:13.989+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:13.990+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:13.991+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:13.992+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:13.995+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:13.996+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:13.999+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:14.000+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:14.002+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:14.003+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:14.004+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:14.005+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:14.007+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:14.008+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:14.009+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:14.010+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:14.012+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:14.013+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:14.013+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:14.014+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:14.016+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:14.017+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:14.018+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: ResultStage 11 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.385 s due to Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 47) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:14.019+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:14.020+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:14.021+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:14.021+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:14.022+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:14.023+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:14.023+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:14.024+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:14.025+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:14.025+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:14.028+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:14.030+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:14.031+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:14.032+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:14.034+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:14.035+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:14.036+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:14.037+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:14.038+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:14.040+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:14.042+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:14.047+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:14.049+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:14.050+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:21.parquet does not exist
[2025-02-04T21:46:14.060+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:14.061+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:14.064+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:14.065+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:14.066+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:14.067+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:14.068+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:14.069+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:14.070+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:14.071+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:14.072+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:14.073+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:14.075+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:14.077+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:14.078+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:14.079+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:14.080+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:14.080+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:14.081+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:14.082+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:14.083+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:14.083+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:14.084+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:14.086+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:14.086+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:14.087+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:14.088+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:14.089+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:14.089+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:14.090+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:13 INFO DAGScheduler: Job 11 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.390279 s
[2025-02-04T21:46:14.167+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 06:21.parquet. Check logs for details.
[2025-02-04T21:46:14.170+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 04:49.parquet
[2025-02-04T21:46:14.183+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:14.226+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:14.228+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: Got job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:14.230+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:14.231+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:14.232+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:14.232+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:14.243+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:14.267+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:14.272+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:14.291+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:14.292+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.300+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.304+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:14.306+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-02-04T21:46:14.308+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 48) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:14.311+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.317+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.318+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.345+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.375+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 48) (172.18.0.5 executor 8): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:14.379+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:14.381+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:14.381+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:14.382+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:14.402+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:14.405+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:14.406+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:14.407+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:14.408+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:14.408+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:14.410+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:14.411+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:14.412+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:14.413+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:14.415+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:14.419+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:14.422+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:14.425+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:14.428+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:14.428+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:14.429+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:14.430+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:14.432+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:14.433+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 04:49.parquet does not exist
[2025-02-04T21:46:14.435+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:14.436+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:14.437+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:14.438+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:14.446+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:14.449+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:14.450+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:14.453+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:14.456+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:14.458+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:14.464+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:14.465+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:14.465+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:14.466+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:14.466+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:14.466+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:14.467+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:14.468+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:14.468+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:14.469+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:14.470+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:14.470+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:14.471+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:14.472+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:14.473+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:14.475+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:14.476+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:14.477+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:14.481+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSetManager: Starting task 0.1 in stage 12.0 (TID 49) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:14.482+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.483+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSetManager: Lost task 0.1 in stage 12.0 (TID 49) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:14.486+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSetManager: Starting task 0.2 in stage 12.0 (TID 50) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:14.503+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.536+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSetManager: Lost task 0.2 in stage 12.0 (TID 50) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:14.540+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSetManager: Starting task 0.3 in stage 12.0 (TID 51) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:14.585+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:14.671+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSetManager: Lost task 0.3 in stage 12.0 (TID 51) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:14.688+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 ERROR TaskSetManager: Task 0 in stage 12.0 failed 4 times; aborting job
[2025-02-04T21:46:14.690+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-02-04T21:46:14.694+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSchedulerImpl: Cancelling stage 12
[2025-02-04T21:46:14.696+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 51) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:14.698+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:14.705+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:14.706+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:14.707+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:14.711+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:14.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:14.714+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:14.715+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:14.716+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:14.722+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:14.726+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:14.728+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:14.732+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:14.735+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:14.736+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:14.738+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:14.740+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:14.741+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:14.742+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:14.743+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:14.748+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:14.748+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:14.750+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:14.752+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 04:49.parquet does not exist
[2025-02-04T21:46:14.753+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:14.755+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:14.755+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:14.756+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:14.758+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:14.761+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:14.762+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:14.763+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:14.764+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:14.765+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:14.768+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:14.770+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:14.770+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:14.771+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:14.773+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:14.774+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:14.775+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:14.777+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:14.778+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:14.779+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:14.781+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:14.782+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:14.783+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:14.787+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:14.788+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:14.808+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:14.810+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:14.811+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:14.812+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:14.814+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: ResultStage 12 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.451 s due to Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 51) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:14.815+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:14.817+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:14.819+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:14.820+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:14.821+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:14.821+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:14.822+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:14.823+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:14.824+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:14.824+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:14.825+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:14.826+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:14.826+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:14.827+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:14.828+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:14.829+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:14.830+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:14.831+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:14.832+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:14.834+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:14.835+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:14.836+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:14.837+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:14.838+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 04:49.parquet does not exist
[2025-02-04T21:46:14.839+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:14.840+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:14.841+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:14.842+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:14.843+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:14.844+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:14.845+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:14.845+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:14.846+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:14.846+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:14.847+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:14.848+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:14.848+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:14.849+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:14.849+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:14.850+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:14.852+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:14.853+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:14.853+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:14.854+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:14.855+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:14.855+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:14.856+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:14.857+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:14.859+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:14.860+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:14.861+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:14.861+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:14.862+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:14.863+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO DAGScheduler: Job 12 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.455415 s
[2025-02-04T21:46:14.952+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 04:49.parquet. Check logs for details.
[2025-02-04T21:46:14.953+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:35.parquet
[2025-02-04T21:46:14.977+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:15.049+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:15.051+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Got job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:15.052+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:15.053+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:15.053+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:15.054+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:15.066+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:15.090+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:15.104+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:15.116+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:15.138+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:15.142+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-02-04T21:46:15.147+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 52) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:15.152+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.158+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.161+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.165+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.168+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.184+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.216+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 52) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:15.222+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:15.222+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:15.227+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:15.230+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:15.234+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:15.234+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:15.235+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:15.236+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:15.237+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:15.238+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:15.239+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:15.240+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:15.240+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:15.241+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:15.243+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:15.244+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:15.246+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:15.247+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:15.249+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:15.251+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:15.254+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:15.255+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:15.260+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:15.262+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:35.parquet does not exist
[2025-02-04T21:46:15.263+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:15.265+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:15.266+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:15.268+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:15.272+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:15.274+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:15.274+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:15.276+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:15.276+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:15.277+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:15.278+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:15.279+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:15.280+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:15.281+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:15.283+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:15.285+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:15.286+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:15.287+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:15.288+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:15.289+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:15.289+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:15.290+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:15.291+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:15.292+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:15.294+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:15.295+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:15.296+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:15.297+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:15.297+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Starting task 0.1 in stage 13.0 (TID 53) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:15.298+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.299+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Lost task 0.1 in stage 13.0 (TID 53) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:15.299+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Starting task 0.2 in stage 13.0 (TID 54) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:15.310+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.364+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Lost task 0.2 in stage 13.0 (TID 54) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:15.365+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Starting task 0.3 in stage 13.0 (TID 55) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:15.422+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Lost task 0.3 in stage 13.0 (TID 55) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:15.423+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 ERROR TaskSetManager: Task 0 in stage 13.0 failed 4 times; aborting job
[2025-02-04T21:46:15.424+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-02-04T21:46:15.427+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSchedulerImpl: Cancelling stage 13
[2025-02-04T21:46:15.429+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 13.0 failed 4 times, most recent failure: Lost task 0.3 in stage 13.0 (TID 55) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:15.430+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:15.432+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:15.433+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:15.435+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:15.438+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:15.439+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:15.441+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:15.442+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:15.443+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:15.444+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:15.445+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:15.446+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:15.447+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:15.450+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:15.452+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:15.453+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:15.454+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:15.456+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:15.457+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:15.457+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:15.458+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:15.459+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:15.461+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:15.462+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:35.parquet does not exist
[2025-02-04T21:46:15.463+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:15.464+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:15.465+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:15.465+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:15.468+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:15.474+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:15.474+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:15.475+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:15.477+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:15.478+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:15.479+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:15.480+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:15.481+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:15.482+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:15.483+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:15.484+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:15.485+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:15.486+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:15.487+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:15.487+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:15.488+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:15.489+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:15.490+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:15.491+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:15.492+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:15.493+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:15.494+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:15.495+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:15.496+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:15.497+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.373 s due to Job aborted due to stage failure: Task 0 in stage 13.0 failed 4 times, most recent failure: Lost task 0.3 in stage 13.0 (TID 55) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:15.498+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:15.499+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:15.501+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:15.502+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:15.503+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:15.505+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:15.505+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:15.506+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:15.508+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:15.509+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:15.510+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:15.511+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:15.511+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:15.512+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:15.512+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:15.513+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:15.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:15.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:15.515+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:15.515+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:15.516+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:15.516+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:15.517+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:15.518+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:35.parquet does not exist
[2025-02-04T21:46:15.519+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:15.520+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:15.520+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:15.521+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:15.522+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:15.522+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:15.523+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:15.524+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:15.524+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:15.525+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:15.526+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:15.527+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:15.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:15.529+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:15.530+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:15.530+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:15.531+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:15.531+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:15.532+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:15.533+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:15.533+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:15.534+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:15.535+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:15.536+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:15.536+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:15.537+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:15.537+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:15.538+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:15.539+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:15.541+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Job 13 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.379784 s
[2025-02-04T21:46:15.758+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:35.parquet. Check logs for details.
[2025-02-04T21:46:15.758+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:57.parquet
[2025-02-04T21:46:15.776+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:15.810+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:15.818+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Got job 14 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:15.819+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Final stage: ResultStage 14 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:15.821+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:15.821+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:15.822+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[29] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:15.828+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:15.838+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:15.845+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:15.853+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_13_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.857+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:15.859+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:15.864+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2025-02-04T21:46:15.866+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.869+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 56) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:15.870+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.870+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.894+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:15.932+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 56) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:15.933+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:15.934+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:15.939+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:15.941+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:15.943+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:15.944+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:15.945+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:15.947+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:15.948+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:15.948+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:15.949+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:15.950+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:15.951+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:15.952+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:15.953+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:15.953+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:15.954+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:15.954+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:15.955+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:15.956+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:15.957+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:15.958+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:15.960+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:15.962+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:57.parquet does not exist
[2025-02-04T21:46:15.962+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:15.963+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:15.964+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:15.965+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:15.965+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:15.966+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:15.967+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:15.969+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:15.970+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:15.971+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:15.971+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:15.972+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:15.973+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:15.974+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:15.975+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:15.977+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:15.977+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:15.978+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:15.979+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:15.980+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:15.980+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:15.981+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:15.982+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:15.983+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:15.984+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:15.985+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:15.986+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:15.987+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:15.989+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO TaskSetManager: Starting task 0.1 in stage 14.0 (TID 57) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:15.990+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.008+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Lost task 0.1 in stage 14.0 (TID 57) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:16.013+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Starting task 0.2 in stage 14.0 (TID 58) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:16.050+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Lost task 0.2 in stage 14.0 (TID 58) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:16.053+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Starting task 0.3 in stage 14.0 (TID 59) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:16.086+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.111+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Lost task 0.3 in stage 14.0 (TID 59) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:16.112+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 ERROR TaskSetManager: Task 0 in stage 14.0 failed 4 times; aborting job
[2025-02-04T21:46:16.113+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-02-04T21:46:16.114+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSchedulerImpl: Cancelling stage 14
[2025-02-04T21:46:16.115+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 59) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:16.116+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:16.117+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:16.119+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:16.120+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:16.121+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:16.122+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:16.123+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:16.125+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:16.126+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:16.128+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:16.130+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:16.131+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:16.132+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:16.134+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:16.136+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:16.137+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:16.138+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:16.143+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:16.144+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:16.145+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:16.146+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:16.147+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:16.148+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:16.149+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:57.parquet does not exist
[2025-02-04T21:46:16.150+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:16.151+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:16.152+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:16.153+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:16.154+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:16.155+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:16.157+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:16.158+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:16.161+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:16.162+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:16.163+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:16.164+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:16.167+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:16.168+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:16.169+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:16.171+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:16.172+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:16.173+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:16.174+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:16.175+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:16.178+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:16.179+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:16.180+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:16.180+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:16.182+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:16.183+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:16.184+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:16.186+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:16.186+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:16.187+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: ResultStage 14 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.295 s due to Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 59) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:16.188+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:16.189+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:16.190+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:16.191+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:16.193+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:16.194+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:16.195+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:16.195+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:16.196+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:16.197+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:16.197+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:16.198+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:16.198+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:16.199+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:16.201+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:16.202+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:16.203+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:16.204+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:16.204+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:16.205+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:16.205+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:16.206+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:16.207+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:16.207+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:57.parquet does not exist
[2025-02-04T21:46:16.208+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:16.209+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:16.210+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:16.211+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:16.211+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:16.211+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:16.212+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:16.212+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:16.214+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:16.214+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:16.215+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:16.215+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:16.215+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:16.216+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:16.216+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:16.216+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:16.217+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:16.218+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:16.218+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:16.218+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:16.219+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:16.219+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:16.220+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:16.220+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:16.220+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:16.221+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:16.221+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:16.221+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:16.222+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:16.226+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Job 14 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.303226 s
[2025-02-04T21:46:16.369+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:57.parquet. Check logs for details.
[2025-02-04T21:46:16.370+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:42.parquet
[2025-02-04T21:46:16.390+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2025-02-04T21:46:16.452+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-02-04T21:46:16.454+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Got job 15 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-02-04T21:46:16.455+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0)
[2025-02-04T21:46:16.456+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:16.457+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:16.463+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[31] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-02-04T21:46:16.464+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:16.471+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:16.477+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:16.478+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.481+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:16.485+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:16.489+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-02-04T21:46:16.495+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.497+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 60) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:16.498+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.499+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.538+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.587+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 60) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:16.588+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:16.589+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:16.589+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:16.590+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:16.592+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:16.595+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:16.597+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:16.598+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:16.600+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:16.601+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:16.602+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:16.603+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:16.604+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:16.607+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:16.608+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:16.609+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:16.611+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:16.613+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:16.614+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:16.615+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:16.616+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:16.617+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:16.618+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:16.619+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:42.parquet does not exist
[2025-02-04T21:46:16.627+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:16.630+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:16.632+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:16.639+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:16.641+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:16.642+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:16.643+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:16.644+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:16.645+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:16.646+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:16.647+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:16.648+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:16.648+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:16.649+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:16.651+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:16.651+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:16.652+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:16.653+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:16.654+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:16.655+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:16.656+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:16.657+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:16.658+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:16.662+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:16.664+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:16.666+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:16.668+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:16.670+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:16.673+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Starting task 0.1 in stage 15.0 (TID 61) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:16.677+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.681+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Lost task 0.1 in stage 15.0 (TID 61) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:16.686+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Starting task 0.2 in stage 15.0 (TID 62) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:16.721+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.748+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Lost task 0.2 in stage 15.0 (TID 62) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:16.750+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Starting task 0.3 in stage 15.0 (TID 63) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:16.795+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:16.879+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSetManager: Lost task 0.3 in stage 15.0 (TID 63) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:16.881+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 ERROR TaskSetManager: Task 0 in stage 15.0 failed 4 times; aborting job
[2025-02-04T21:46:16.885+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-02-04T21:46:16.887+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSchedulerImpl: Cancelling stage 15
[2025-02-04T21:46:16.889+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 63) (172.18.0.5 executor 7): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:16.890+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:16.891+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:16.894+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:16.896+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:16.898+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:16.901+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:16.903+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:16.907+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:16.908+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:16.909+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:16.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:16.911+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:16.913+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:16.915+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:16.918+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:16.921+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:16.921+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:16.923+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:16.923+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:16.925+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:16.926+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:16.928+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:16.929+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:16.930+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:42.parquet does not exist
[2025-02-04T21:46:16.932+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:16.933+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:16.935+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:16.936+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:16.939+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:16.941+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:16.942+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:16.943+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:16.944+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:16.946+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:16.946+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:16.947+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:16.950+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:16.951+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:16.952+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:16.953+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:16.954+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:16.955+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:16.956+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:16.957+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:16.958+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:16.960+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:16.962+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:16.963+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:16.964+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:16.965+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:16.965+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:16.967+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:16.968+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:16.969+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0) failed in 0.430 s due to Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 63) (172.18.0.5 executor 7): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:16.970+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:16.971+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:16.973+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:16.974+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:16.976+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:16.977+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:16.978+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:16.980+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:16.981+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:16.984+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:16.986+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:16.989+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:16.993+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:17.007+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:17.008+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:17.009+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:17.010+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:17.012+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:17.014+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:17.015+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:17.016+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:17.017+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:17.018+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:17.019+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:42.parquet does not exist
[2025-02-04T21:46:17.019+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:17.020+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:17.021+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:17.022+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:17.024+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:17.025+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:17.026+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:17.027+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:17.028+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:17.029+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:17.030+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:17.031+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:17.032+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:17.035+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:17.037+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:17.038+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:17.039+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:17.040+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:17.042+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:17.043+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:17.044+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:17.045+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:17.045+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:17.046+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:17.047+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:17.047+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:17.048+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:17.048+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:17.049+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:17.050+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:16 INFO DAGScheduler: Job 15 failed: parquet at NativeMethodAccessorImpl.java:0, took 0.436621 s
[2025-02-04T21:46:17.152+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:42.parquet. Check logs for details.
[2025-02-04T21:46:17.152+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 06:24.parquet
[2025-02-04T21:46:17.180+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:17.228+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:17.235+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: Got job 16 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:17.244+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: Final stage: ResultStage 16 (parquet at <unknown>:0)
[2025-02-04T21:46:17.251+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:17.254+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:17.254+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[33] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:17.255+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:17.271+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:17.285+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Removed broadcast_15_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.286+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.288+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.289+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.291+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:17.294+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:17.295+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-02-04T21:46:17.296+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 64) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:17.301+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.308+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.341+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.377+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 64) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:17.380+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:17.382+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:17.385+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:17.388+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:17.389+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:17.394+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:17.395+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:17.398+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:17.402+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:17.403+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:17.406+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:17.410+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:17.411+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:17.412+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:17.414+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:17.415+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:17.415+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:17.417+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:17.419+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:17.420+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:17.420+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:17.421+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:17.421+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:17.422+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:24.parquet does not exist
[2025-02-04T21:46:17.423+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:17.426+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:17.427+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:17.428+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:17.428+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:17.432+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:17.433+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:17.434+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:17.435+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:17.436+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:17.437+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:17.437+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:17.438+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:17.439+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:17.440+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:17.443+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:17.444+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:17.444+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:17.445+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:17.448+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:17.449+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:17.450+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:17.451+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:17.452+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:17.453+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:17.454+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:17.454+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:17.455+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:17.456+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSetManager: Starting task 0.1 in stage 16.0 (TID 65) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:17.457+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.457+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSetManager: Lost task 0.1 in stage 16.0 (TID 65) on 172.18.0.5, executor 10: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:17.458+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSetManager: Starting task 0.2 in stage 16.0 (TID 66) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:17.490+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:17.538+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSetManager: Lost task 0.2 in stage 16.0 (TID 66) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:17.556+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSetManager: Starting task 0.3 in stage 16.0 (TID 67) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:17.618+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSetManager: Lost task 0.3 in stage 16.0 (TID 67) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:17.620+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 ERROR TaskSetManager: Task 0 in stage 16.0 failed 4 times; aborting job
[2025-02-04T21:46:17.623+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-02-04T21:46:17.626+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSchedulerImpl: Cancelling stage 16
[2025-02-04T21:46:17.627+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 16.0 failed 4 times, most recent failure: Lost task 0.3 in stage 16.0 (TID 67) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:17.628+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:17.629+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:17.632+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:17.634+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:17.661+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:17.664+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:17.666+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:17.668+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:17.669+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:17.670+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:17.672+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:17.681+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:17.682+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:17.684+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:17.685+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:17.687+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:17.688+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:17.689+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:17.690+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:17.692+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:17.693+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:17.694+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:17.697+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:17.699+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:24.parquet does not exist
[2025-02-04T21:46:17.700+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:17.701+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:17.702+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:17.703+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:17.704+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:17.705+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:17.706+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:17.706+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:17.707+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:17.710+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:17.710+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:17.711+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:17.712+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:17.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:17.713+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:17.714+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:17.715+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:17.715+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:17.716+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:17.717+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:17.718+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:17.719+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:17.720+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:17.721+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:17.722+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:17.722+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:17.723+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:17.724+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:17.724+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:17.725+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: ResultStage 16 (parquet at <unknown>:0) failed in 0.382 s due to Job aborted due to stage failure: Task 0 in stage 16.0 failed 4 times, most recent failure: Lost task 0.3 in stage 16.0 (TID 67) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:17.726+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:17.727+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:17.728+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:17.728+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:17.729+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:17.730+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:17.730+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:17.731+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:17.731+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:17.732+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:17.733+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:17.734+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:17.734+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:17.735+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:17.736+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:17.736+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:17.737+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:17.738+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:17.739+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:17.740+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:17.741+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:17.742+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:17.742+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:17.743+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:24.parquet does not exist
[2025-02-04T21:46:17.744+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:17.748+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:17.749+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:17.750+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:17.751+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:17.752+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:17.753+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:17.754+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:17.755+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:17.755+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:17.756+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:17.756+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:17.757+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:17.758+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:17.758+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:17.759+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:17.760+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:17.761+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:17.762+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:17.762+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:17.764+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:17.765+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:17.766+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:17.766+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:17.767+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:17.769+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:17.770+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:17.771+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:17.771+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:17.772+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO DAGScheduler: Job 16 failed: parquet at <unknown>:0, took 0.392140 s
[2025-02-04T21:46:17.936+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 06:24.parquet. Check logs for details.
[2025-02-04T21:46:17.937+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:29.parquet
[2025-02-04T21:46:17.958+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:17 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:18.045+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:18.051+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Got job 17 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:18.056+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Final stage: ResultStage 17 (parquet at <unknown>:0)
[2025-02-04T21:46:18.057+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:18.064+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:18.070+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[35] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:18.078+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:18.107+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:18.116+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:18.126+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_16_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.138+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:18.139+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:18.143+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-02-04T21:46:18.149+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 68) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:18.151+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.160+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.179+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.196+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.236+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 68) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:18.237+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:18.241+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:18.268+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:18.269+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:18.274+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:18.275+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:18.278+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:18.279+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:18.280+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:18.282+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:18.283+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:18.285+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:18.286+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:18.286+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:18.287+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:18.289+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:18.290+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:18.290+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:18.291+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:18.294+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:18.295+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:18.296+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:18.298+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:18.300+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:29.parquet does not exist
[2025-02-04T21:46:18.301+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:18.302+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:18.303+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:18.304+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:18.306+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:18.307+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:18.309+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:18.310+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:18.311+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:18.312+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:18.313+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:18.314+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:18.315+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:18.316+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:18.318+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:18.319+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:18.321+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:18.321+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:18.324+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:18.325+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:18.326+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:18.327+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:18.328+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:18.330+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:18.331+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:18.332+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:18.333+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:18.334+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:18.335+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Starting task 0.1 in stage 17.0 (TID 69) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:18.336+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.337+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Lost task 0.1 in stage 17.0 (TID 69) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:18.338+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Starting task 0.2 in stage 17.0 (TID 70) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:18.366+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.401+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Lost task 0.2 in stage 17.0 (TID 70) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:18.403+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Starting task 0.3 in stage 17.0 (TID 71) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:18.433+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.459+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Lost task 0.3 in stage 17.0 (TID 71) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:18.462+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 ERROR TaskSetManager: Task 0 in stage 17.0 failed 4 times; aborting job
[2025-02-04T21:46:18.463+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-02-04T21:46:18.464+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSchedulerImpl: Cancelling stage 17
[2025-02-04T21:46:18.465+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 17.0 failed 4 times, most recent failure: Lost task 0.3 in stage 17.0 (TID 71) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:18.465+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:18.467+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:18.468+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:18.469+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:18.469+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:18.470+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:18.472+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:18.474+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:18.475+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:18.480+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:18.486+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:18.490+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:18.491+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:18.493+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:18.495+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:18.496+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:18.497+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:18.498+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:18.499+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:18.500+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:18.501+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:18.502+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:18.503+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:18.505+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:29.parquet does not exist
[2025-02-04T21:46:18.506+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:18.507+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:18.507+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:18.508+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:18.508+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:18.509+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:18.509+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:18.510+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:18.510+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:18.510+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:18.512+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:18.513+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:18.513+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:18.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:18.514+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:18.514+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:18.515+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:18.515+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:18.516+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:18.517+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:18.517+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:18.518+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:18.518+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:18.519+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:18.519+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:18.520+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:18.520+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:18.520+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:18.521+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:18.521+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: ResultStage 17 (parquet at <unknown>:0) failed in 0.403 s due to Job aborted due to stage failure: Task 0 in stage 17.0 failed 4 times, most recent failure: Lost task 0.3 in stage 17.0 (TID 71) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:18.522+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:18.522+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:18.522+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:18.523+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:18.523+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:18.524+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:18.524+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:18.525+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:18.525+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:18.526+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:18.526+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:18.527+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:18.527+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:18.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:18.529+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:18.529+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:18.530+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:18.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:18.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:18.532+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:18.532+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:18.533+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:18.534+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:18.535+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:29.parquet does not exist
[2025-02-04T21:46:18.535+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:18.536+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:18.537+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:18.537+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:18.538+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:18.538+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:18.539+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:18.540+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:18.541+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:18.542+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:18.543+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:18.543+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:18.544+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:18.545+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:18.545+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:18.546+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:18.546+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:18.546+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:18.547+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:18.548+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:18.548+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:18.549+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:18.550+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:18.551+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:18.551+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:18.552+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:18.552+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:18.553+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:18.553+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:18.554+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Job 17 failed: parquet at <unknown>:0, took 0.415251 s
[2025-02-04T21:46:18.703+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:29.parquet. Check logs for details.
[2025-02-04T21:46:18.704+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:39.parquet
[2025-02-04T21:46:18.723+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:18.761+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:18.763+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Got job 18 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:18.764+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Final stage: ResultStage 18 (parquet at <unknown>:0)
[2025-02-04T21:46:18.764+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:18.765+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:18.766+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[37] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:18.775+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:18.792+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:18.798+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:18.808+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:18.819+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:18.822+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-02-04T21:46:18.823+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 72) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:18.825+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.833+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_17_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.840+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.845+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.846+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.879+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:18.909+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 72) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:18.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:18.912+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:18.915+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:18.917+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:18.918+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:18.920+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:18.921+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:18.923+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:18.924+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:18.926+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:18.930+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:18.931+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:18.932+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:18.934+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:18.935+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:18.935+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:18.944+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:18.945+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:18.951+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:18.952+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:18.953+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:18.955+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:18.956+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:18.957+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:39.parquet does not exist
[2025-02-04T21:46:18.959+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:18.965+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:18.966+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:18.966+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:18.968+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:18.970+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:18.971+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:18.972+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:18.976+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:18.977+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:18.978+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:18.978+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:18.980+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:18.981+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:18.981+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:18.982+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:18.983+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:18.984+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:18.985+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:18.987+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:18.988+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:18.989+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:18.990+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:18.991+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:18.993+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:18.994+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:18.997+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:19.001+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:19.002+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Starting task 0.1 in stage 18.0 (TID 73) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:19.004+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Lost task 0.1 in stage 18.0 (TID 73) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:19.005+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:18 INFO TaskSetManager: Starting task 0.2 in stage 18.0 (TID 74) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:19.006+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.036+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Lost task 0.2 in stage 18.0 (TID 74) on 172.18.0.5, executor 8: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:19.040+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Starting task 0.3 in stage 18.0 (TID 75) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:19.074+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.103+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Lost task 0.3 in stage 18.0 (TID 75) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:19.106+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 ERROR TaskSetManager: Task 0 in stage 18.0 failed 4 times; aborting job
[2025-02-04T21:46:19.106+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-02-04T21:46:19.107+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSchedulerImpl: Cancelling stage 18
[2025-02-04T21:46:19.108+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 75) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:19.111+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:19.112+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:19.112+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:19.113+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:19.114+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:19.117+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:19.119+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:19.120+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:19.121+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:19.123+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:19.124+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:19.127+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:19.128+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:19.130+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:19.131+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:19.133+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:19.134+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:19.137+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:19.138+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:19.157+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:19.163+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:19.172+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:19.173+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:19.175+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:39.parquet does not exist
[2025-02-04T21:46:19.177+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:19.178+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:19.180+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:19.181+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:19.182+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:19.185+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:19.186+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:19.187+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:19.189+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:19.190+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:19.192+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:19.193+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:19.194+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:19.195+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:19.196+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:19.197+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:19.198+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:19.198+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:19.200+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:19.201+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:19.203+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:19.210+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:19.211+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:19.212+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:19.214+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:19.215+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:19.216+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:19.218+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:19.218+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:19.219+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: ResultStage 18 (parquet at <unknown>:0) failed in 0.339 s due to Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 75) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:19.220+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:19.221+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:19.223+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:19.224+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:19.225+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:19.228+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:19.230+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:19.231+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:19.232+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:19.234+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:19.234+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:19.235+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:19.236+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:19.237+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:19.238+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:19.241+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:19.242+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:19.243+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:19.244+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:19.245+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:19.246+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:19.247+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:19.248+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:19.249+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:39.parquet does not exist
[2025-02-04T21:46:19.251+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:19.251+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:19.252+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:19.253+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:19.255+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:19.255+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:19.256+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:19.257+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:19.258+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:19.259+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:19.260+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:19.262+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:19.263+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:19.264+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:19.265+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:19.267+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:19.268+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:19.269+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:19.270+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:19.272+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:19.273+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:19.274+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:19.275+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:19.276+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:19.277+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:19.278+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:19.278+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:19.279+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:19.280+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:19.281+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Job 18 failed: parquet at <unknown>:0, took 0.344062 s
[2025-02-04T21:46:19.406+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:39.parquet. Check logs for details.
[2025-02-04T21:46:19.407+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 04:55.parquet
[2025-02-04T21:46:19.423+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:19.476+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:19.477+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Got job 19 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:19.480+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Final stage: ResultStage 19 (parquet at <unknown>:0)
[2025-02-04T21:46:19.481+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:19.482+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:19.483+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:19.492+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:19.502+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:19.506+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:19.512+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:19.514+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:19.518+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-02-04T21:46:19.521+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Removed broadcast_18_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.524+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 76) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:19.531+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.535+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.543+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.577+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.609+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 76) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:19.610+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:19.611+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:19.612+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:19.613+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:19.615+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:19.618+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:19.619+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:19.622+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:19.623+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:19.625+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:19.626+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:19.627+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:19.628+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:19.630+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:19.631+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:19.633+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:19.636+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:19.638+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:19.638+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:19.639+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:19.644+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:19.646+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:19.648+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:19.649+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 04:55.parquet does not exist
[2025-02-04T21:46:19.651+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:19.653+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:19.658+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:19.659+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:19.660+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:19.663+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:19.665+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:19.667+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:19.668+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:19.669+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:19.671+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:19.672+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:19.673+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:19.674+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:19.677+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:19.678+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:19.680+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:19.682+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:19.683+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:19.687+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:19.693+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:19.701+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:19.707+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:19.709+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:19.711+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:19.713+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:19.716+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:19.719+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:19.721+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Starting task 0.1 in stage 19.0 (TID 77) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:19.730+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.732+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Lost task 0.1 in stage 19.0 (TID 77) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:19.739+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Starting task 0.2 in stage 19.0 (TID 78) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:19.780+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.817+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Lost task 0.2 in stage 19.0 (TID 78) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:19.819+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Starting task 0.3 in stage 19.0 (TID 79) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:19.854+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:19.903+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSetManager: Lost task 0.3 in stage 19.0 (TID 79) on 172.18.0.6, executor 1: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:19.904+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 ERROR TaskSetManager: Task 0 in stage 19.0 failed 4 times; aborting job
[2025-02-04T21:46:19.905+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-02-04T21:46:19.905+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSchedulerImpl: Cancelling stage 19
[2025-02-04T21:46:19.907+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 79) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:19.919+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:19.920+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:19.923+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:19.924+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:19.925+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:19.926+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:19.927+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:19.928+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:19.931+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:19.934+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:19.937+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:19.938+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:19.939+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:19.944+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:19.945+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:19.947+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:19.948+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:19.949+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:19.950+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:19.951+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:19.952+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:19.953+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:19.954+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:19.956+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 04:55.parquet does not exist
[2025-02-04T21:46:19.960+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:19.961+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:19.962+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:19.968+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:19.969+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:19.970+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:19.972+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:19.973+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:19.975+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:19.977+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:19.978+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:19.979+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:19.980+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:19.983+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:20.037+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:20.038+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:20.040+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:20.053+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:20.054+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:20.074+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:20.077+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:20.080+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:20.097+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:20.098+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:20.100+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:20.104+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:20.105+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:20.106+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:20.107+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:20.108+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: ResultStage 19 (parquet at <unknown>:0) failed in 0.421 s due to Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 79) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:20.109+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:20.110+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:20.111+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:20.112+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:20.114+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:20.114+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:20.115+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:20.116+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:20.118+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:20.118+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:20.119+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:20.120+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:20.122+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:20.123+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:20.124+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:20.125+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:20.126+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:20.128+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:20.129+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:20.130+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:20.131+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:20.132+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:20.134+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:20.134+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 04:55.parquet does not exist
[2025-02-04T21:46:20.135+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:20.136+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:20.137+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:20.137+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:20.138+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:20.139+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:20.140+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:20.140+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:20.142+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:20.143+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:20.143+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:20.144+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:20.145+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:20.146+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:20.147+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:20.148+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:20.150+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:20.151+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:20.151+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:20.152+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:20.153+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:20.153+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:20.154+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:20.155+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:20.155+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:20.156+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:20.157+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:20.157+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:20.158+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:20.159+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:19 INFO DAGScheduler: Job 19 failed: parquet at <unknown>:0, took 0.427787 s
[2025-02-04T21:46:20.238+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 04:55.parquet. Check logs for details.
[2025-02-04T21:46:20.239+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 05:16.parquet
[2025-02-04T21:46:20.268+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:20.309+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:20.311+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: Got job 20 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:20.312+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: Final stage: ResultStage 20 (parquet at <unknown>:0)
[2025-02-04T21:46:20.317+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:20.318+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:20.318+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[41] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:20.323+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:20.334+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:20.344+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:20.347+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.352+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:20.354+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:20.356+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2025-02-04T21:46:20.360+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.363+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.367+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.368+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 80) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:20.374+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.409+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.438+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 WARN TaskSetManager: Lost task 0.0 in stage 20.0 (TID 80) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:20.442+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:20.443+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:20.449+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:20.498+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:20.501+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:20.507+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:20.508+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:20.510+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:20.511+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:20.512+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:20.513+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:20.515+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:20.517+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:20.519+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:20.521+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:20.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:20.528+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:20.529+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:20.530+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:20.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:20.533+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:20.534+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:20.535+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:20.536+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 05:16.parquet does not exist
[2025-02-04T21:46:20.537+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:20.538+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:20.539+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:20.540+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:20.547+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:20.548+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:20.549+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:20.550+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:20.551+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:20.552+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:20.553+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:20.555+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:20.557+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:20.560+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:20.561+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:20.564+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:20.567+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:20.570+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:20.572+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:20.573+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:20.574+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:20.578+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:20.579+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:20.584+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:20.586+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:20.588+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:20.590+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:20.591+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:20.596+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSetManager: Starting task 0.1 in stage 20.0 (TID 81) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:20.600+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.601+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSetManager: Lost task 0.1 in stage 20.0 (TID 81) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:20.604+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSetManager: Starting task 0.2 in stage 20.0 (TID 82) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:20.662+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.6:38973 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.703+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSetManager: Lost task 0.2 in stage 20.0 (TID 82) on 172.18.0.6, executor 3: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:20.706+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSetManager: Starting task 0.3 in stage 20.0 (TID 83) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:20.767+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:20.801+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSetManager: Lost task 0.3 in stage 20.0 (TID 83) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:20.802+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 ERROR TaskSetManager: Task 0 in stage 20.0 failed 4 times; aborting job
[2025-02-04T21:46:20.802+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-02-04T21:46:20.804+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSchedulerImpl: Cancelling stage 20
[2025-02-04T21:46:20.805+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 20.0 failed 4 times, most recent failure: Lost task 0.3 in stage 20.0 (TID 83) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:20.808+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:20.816+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:20.821+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:20.823+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:20.826+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:20.827+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:20.828+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:20.829+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:20.830+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:20.832+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:20.833+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:20.834+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:20.835+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:20.836+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:20.837+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:20.839+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:20.841+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:20.843+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:20.844+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:20.846+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:20.848+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:20.858+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:20.861+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:20.862+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 05:16.parquet does not exist
[2025-02-04T21:46:20.863+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:20.864+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:20.864+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:20.865+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:20.866+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:20.867+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:20.867+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:20.868+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:20.869+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:20.869+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:20.870+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:20.870+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:20.871+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:20.872+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:20.872+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:20.873+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:20.875+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:20.876+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:20.876+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:20.877+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:20.877+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:20.878+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:20.878+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:20.879+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:20.879+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:20.880+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:20.880+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:20.881+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:20.882+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:20.883+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: ResultStage 20 (parquet at <unknown>:0) failed in 0.492 s due to Job aborted due to stage failure: Task 0 in stage 20.0 failed 4 times, most recent failure: Lost task 0.3 in stage 20.0 (TID 83) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:20.884+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:20.885+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:20.885+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:20.886+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:20.887+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:20.887+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:20.888+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:20.888+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:20.889+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:20.890+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:20.891+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:20.892+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:20.893+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:20.893+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:20.894+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:20.895+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:20.896+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:20.897+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:20.897+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:20.898+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:20.899+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:20.900+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:20.901+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:20.902+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 05:16.parquet does not exist
[2025-02-04T21:46:20.902+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:20.903+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:20.904+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:20.904+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:20.905+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:20.906+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:20.906+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:20.907+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:20.909+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:20.909+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:20.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:20.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:20.911+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:20.912+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:20.912+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:20.913+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:20.913+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:20.914+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:20.917+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:20.918+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:20.920+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:20.925+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:20.926+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:20.931+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:20.934+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:20.935+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:20.939+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:20.942+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:20.945+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:20.949+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:20 INFO DAGScheduler: Job 20 failed: parquet at <unknown>:0, took 0.496375 s
[2025-02-04T21:46:21.271+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 05:16.parquet. Check logs for details.
[2025-02-04T21:46:21.273+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 12:02.parquet
[2025-02-04T21:46:21.291+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:21.336+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:21.337+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: Got job 21 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:21.338+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at <unknown>:0)
[2025-02-04T21:46:21.339+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:21.339+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:21.340+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[43] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:21.354+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:21.374+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:21.377+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:21.406+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:21.409+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:21.411+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2025-02-04T21:46:21.415+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 84) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:21.418+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Removed broadcast_20_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.425+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.429+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.435+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.6:38973 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.450+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.457+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.508+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 84) (172.18.0.5 executor 8): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:21.509+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:21.510+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:21.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:21.519+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:21.524+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:21.527+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:21.529+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:21.531+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:21.533+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:21.533+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:21.539+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:21.539+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:21.540+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:21.543+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:21.544+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:21.549+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:21.549+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:21.553+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:21.555+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:21.555+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:21.560+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:21.563+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:21.564+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:21.565+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:02.parquet does not exist
[2025-02-04T21:46:21.569+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:21.570+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:21.571+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:21.573+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:21.574+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:21.578+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:21.582+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:21.583+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:21.585+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:21.586+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:21.587+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:21.588+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:21.589+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:21.590+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:21.591+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:21.592+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:21.593+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:21.594+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:21.595+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:21.596+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:21.598+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:21.599+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:21.600+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:21.601+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:21.602+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:21.603+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:21.604+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:21.605+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:21.606+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSetManager: Starting task 0.1 in stage 21.0 (TID 85) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:21.608+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.615+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSetManager: Lost task 0.1 in stage 21.0 (TID 85) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:21.616+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSetManager: Starting task 0.2 in stage 21.0 (TID 86) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:21.685+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:21.728+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSetManager: Lost task 0.2 in stage 21.0 (TID 86) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:21.730+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSetManager: Starting task 0.3 in stage 21.0 (TID 87) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:21.761+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSetManager: Lost task 0.3 in stage 21.0 (TID 87) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:21.762+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 ERROR TaskSetManager: Task 0 in stage 21.0 failed 4 times; aborting job
[2025-02-04T21:46:21.762+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2025-02-04T21:46:21.765+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSchedulerImpl: Cancelling stage 21
[2025-02-04T21:46:21.766+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 21.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21.0 (TID 87) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:21.767+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:21.767+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:21.768+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:21.769+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:21.771+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:21.772+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:21.773+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:21.774+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:21.775+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:21.776+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:21.777+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:21.778+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:21.779+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:21.779+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:21.780+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:21.781+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:21.782+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:21.782+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:21.783+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:21.784+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:21.786+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:21.787+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:21.788+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:21.788+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:02.parquet does not exist
[2025-02-04T21:46:21.789+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:21.790+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:21.791+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:21.792+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:21.793+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:21.794+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:21.795+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:21.795+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:21.796+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:21.801+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:21.803+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:21.804+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:21.808+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:21.809+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:21.813+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:21.814+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:21.815+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:21.820+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:21.821+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:21.822+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:21.822+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:21.826+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:21.827+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:21.828+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:21.829+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:21.830+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:21.831+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:21.832+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:21.833+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:21.835+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: ResultStage 21 (parquet at <unknown>:0) failed in 0.425 s due to Job aborted due to stage failure: Task 0 in stage 21.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21.0 (TID 87) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:21.837+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:21.838+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:21.839+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:21.840+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:21.841+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:21.842+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:21.842+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:21.843+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:21.844+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:21.844+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:21.845+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:21.846+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:21.847+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:21.847+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:21.849+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:21.851+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:21.852+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:21.854+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:21.856+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:21.857+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:21.858+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:21.860+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:21.863+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:21.864+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:02.parquet does not exist
[2025-02-04T21:46:21.865+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:21.866+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:21.867+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:21.867+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:21.868+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:21.868+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:21.869+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:21.869+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:21.870+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:21.871+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:21.871+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:21.872+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:21.873+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:21.873+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:21.874+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:21.874+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:21.875+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:21.876+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:21.877+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:21.878+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:21.879+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:21.880+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:21.881+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:21.882+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:21.882+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:21.883+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:21.884+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:21.885+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:21.886+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:21.887+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO DAGScheduler: Job 21 failed: parquet at <unknown>:0, took 0.430455 s
[2025-02-04T21:46:21.984+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 12:02.parquet. Check logs for details.
[2025-02-04T21:46:21.985+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:58.parquet
[2025-02-04T21:46:21.999+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:21 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:22.029+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:22.031+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Got job 22 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:22.032+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Final stage: ResultStage 22 (parquet at <unknown>:0)
[2025-02-04T21:46:22.034+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:22.034+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:22.035+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[45] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:22.044+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:22.053+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:22.062+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:22.064+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_21_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.066+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:22.068+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:22.076+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-02-04T21:46:22.079+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 88) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:22.089+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.093+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.147+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.158+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.195+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 WARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 88) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:22.196+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:22.196+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:22.199+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:22.200+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:22.200+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:22.203+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:22.204+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:22.204+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:22.205+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:22.207+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:22.208+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:22.210+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:22.211+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:22.212+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:22.212+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:22.214+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:22.215+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:22.216+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:22.217+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:22.218+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:22.219+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:22.220+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:22.221+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:22.223+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:58.parquet does not exist
[2025-02-04T21:46:22.223+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:22.224+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:22.225+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:22.226+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:22.227+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:22.228+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:22.228+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:22.229+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:22.230+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:22.230+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:22.231+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:22.232+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:22.233+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:22.234+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:22.234+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:22.235+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:22.235+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:22.236+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:22.237+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:22.237+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:22.238+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:22.240+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:22.240+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:22.242+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:22.242+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:22.243+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:22.244+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:22.245+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:22.246+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.1 in stage 22.0 (TID 89) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:22.248+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.250+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Lost task 0.1 in stage 22.0 (TID 89) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:22.251+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.2 in stage 22.0 (TID 90) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:22.275+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.306+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Lost task 0.2 in stage 22.0 (TID 90) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:22.311+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.3 in stage 22.0 (TID 91) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:22.354+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.385+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Lost task 0.3 in stage 22.0 (TID 91) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:22.393+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 ERROR TaskSetManager: Task 0 in stage 22.0 failed 4 times; aborting job
[2025-02-04T21:46:22.396+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-02-04T21:46:22.399+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSchedulerImpl: Cancelling stage 22
[2025-02-04T21:46:22.401+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 22.0 failed 4 times, most recent failure: Lost task 0.3 in stage 22.0 (TID 91) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:22.405+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:22.407+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:22.409+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:22.412+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:22.418+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:22.420+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:22.421+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:22.424+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:22.430+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:22.433+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:22.435+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:22.437+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:22.438+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:22.440+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:22.442+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:22.443+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:22.444+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:22.444+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:22.445+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:22.446+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:22.447+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:22.448+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:22.449+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:22.449+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:58.parquet does not exist
[2025-02-04T21:46:22.450+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:22.451+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:22.451+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:22.452+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:22.452+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:22.453+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:22.454+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:22.454+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:22.455+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:22.456+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:22.457+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:22.458+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:22.459+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:22.459+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:22.460+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:22.460+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:22.463+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:22.464+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:22.464+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:22.465+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:22.466+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:22.467+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:22.468+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:22.469+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:22.470+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:22.470+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:22.471+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:22.472+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:22.473+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:22.473+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: ResultStage 22 (parquet at <unknown>:0) failed in 0.357 s due to Job aborted due to stage failure: Task 0 in stage 22.0 failed 4 times, most recent failure: Lost task 0.3 in stage 22.0 (TID 91) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:22.474+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:22.476+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:22.477+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:22.478+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:22.479+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:22.480+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:22.480+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:22.481+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:22.481+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:22.482+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:22.483+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:22.485+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:22.485+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:22.487+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:22.488+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:22.489+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:22.490+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:22.491+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:22.493+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:22.497+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:22.498+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:22.499+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:22.502+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:22.503+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:58.parquet does not exist
[2025-02-04T21:46:22.504+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:22.505+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:22.506+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:22.507+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:22.509+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:22.510+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:22.511+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:22.514+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:22.517+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:22.518+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:22.519+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:22.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:22.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:22.521+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:22.522+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:22.522+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:22.523+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:22.524+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:22.524+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:22.526+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:22.527+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:22.528+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:22.529+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:22.530+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:22.532+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:22.534+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:22.538+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:22.545+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:22.547+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:22.548+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Job 22 failed: parquet at <unknown>:0, took 0.361559 s
[2025-02-04T21:46:22.684+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:58.parquet. Check logs for details.
[2025-02-04T21:46:22.684+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:51.parquet
[2025-02-04T21:46:22.705+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2025-02-04T21:46:22.758+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:22.760+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Got job 23 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:22.762+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Final stage: ResultStage 23 (parquet at <unknown>:0)
[2025-02-04T21:46:22.763+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:22.764+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:22.764+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[47] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:22.771+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:22.782+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:22.790+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:22.792+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.797+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:22.813+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:22.815+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2025-02-04T21:46:22.819+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 92) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:22.825+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.825+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.826+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.827+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.835+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.866+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 92) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:22.867+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:22.869+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:22.870+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:22.870+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:22.871+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:22.872+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:22.873+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:22.874+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:22.875+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:22.876+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:22.881+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:22.883+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:22.884+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:22.885+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:22.886+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:22.887+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:22.887+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:22.888+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:22.889+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:22.891+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:22.892+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:22.894+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:22.896+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:22.901+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:51.parquet does not exist
[2025-02-04T21:46:22.902+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:22.903+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:22.904+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:22.905+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:22.906+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:22.906+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:22.907+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:22.908+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:22.909+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:22.910+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:22.911+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:22.912+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:22.912+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:22.913+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:22.913+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:22.914+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:22.914+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:22.917+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:22.919+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:22.920+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:22.921+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:22.921+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:22.923+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:22.926+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:22.927+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:22.928+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:22.929+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:22.929+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:22.931+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.1 in stage 23.0 (TID 93) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:22.933+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.934+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Lost task 0.1 in stage 23.0 (TID 93) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:22.935+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.2 in stage 23.0 (TID 94) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:22.962+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:22.996+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Lost task 0.2 in stage 23.0 (TID 94) on 172.18.0.5, executor 10: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:22.997+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:22 INFO TaskSetManager: Starting task 0.3 in stage 23.0 (TID 95) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:23.036+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.068+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Lost task 0.3 in stage 23.0 (TID 95) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:23.069+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 ERROR TaskSetManager: Task 0 in stage 23.0 failed 4 times; aborting job
[2025-02-04T21:46:23.070+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-02-04T21:46:23.071+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSchedulerImpl: Cancelling stage 23
[2025-02-04T21:46:23.073+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 95) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:23.075+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:23.077+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:23.081+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:23.083+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:23.083+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:23.084+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:23.085+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:23.086+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:23.087+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:23.088+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:23.089+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:23.090+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:23.091+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:23.092+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:23.093+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:23.094+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:23.095+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:23.097+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:23.098+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:23.100+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:23.100+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:23.101+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:23.103+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:23.104+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:51.parquet does not exist
[2025-02-04T21:46:23.104+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:23.105+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:23.106+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:23.107+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:23.107+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:23.108+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:23.109+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:23.110+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:23.111+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:23.112+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:23.113+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:23.114+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:23.116+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:23.117+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:23.118+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:23.118+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:23.119+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:23.120+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:23.120+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:23.121+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:23.122+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:23.122+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:23.123+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:23.124+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:23.125+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:23.125+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:23.126+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:23.127+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:23.128+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:23.129+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: ResultStage 23 (parquet at <unknown>:0) failed in 0.308 s due to Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 95) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:23.130+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:23.131+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:23.133+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:23.134+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:23.134+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:23.135+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:23.136+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:23.137+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:23.138+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:23.139+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:23.140+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:23.141+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:23.142+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:23.143+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:23.143+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:23.144+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:23.146+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:23.146+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:23.147+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:23.149+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:23.150+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:23.151+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:23.151+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:23.152+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:51.parquet does not exist
[2025-02-04T21:46:23.152+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:23.153+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:23.154+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:23.154+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:23.155+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:23.155+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:23.156+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:23.158+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:23.160+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:23.160+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:23.161+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:23.163+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:23.164+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:23.164+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:23.165+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:23.167+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:23.168+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:23.169+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:23.170+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:23.170+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:23.171+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:23.172+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:23.173+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:23.176+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:23.177+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:23.180+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:23.181+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:23.183+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:23.184+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:23.185+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Job 23 failed: parquet at <unknown>:0, took 0.312484 s
[2025-02-04T21:46:23.359+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:51.parquet. Check logs for details.
[2025-02-04T21:46:23.359+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:41.parquet
[2025-02-04T21:46:23.371+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:23.404+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:23.406+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Got job 24 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:23.407+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Final stage: ResultStage 24 (parquet at <unknown>:0)
[2025-02-04T21:46:23.409+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:23.409+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:23.410+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[49] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:23.417+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:23.430+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:23.437+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:23.438+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:23.441+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:23.442+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2025-02-04T21:46:23.444+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 96) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:23.448+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.465+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.467+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.468+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.479+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.509+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.535+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 96) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:23.536+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:23.537+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:23.538+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:23.539+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:23.540+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:23.541+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:23.542+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:23.543+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:23.544+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:23.545+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:23.546+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:23.548+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:23.550+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:23.551+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:23.552+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:23.553+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:23.554+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:23.555+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:23.555+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:23.556+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:23.556+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:23.557+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:23.558+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:23.559+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:41.parquet does not exist
[2025-02-04T21:46:23.560+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:23.561+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:23.561+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:23.563+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:23.563+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:23.564+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:23.565+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:23.567+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:23.567+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:23.569+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:23.569+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:23.570+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:23.571+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:23.571+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:23.572+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:23.573+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:23.574+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:23.575+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:23.577+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:23.578+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:23.579+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:23.579+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:23.580+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:23.581+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:23.584+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:23.585+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:23.585+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:23.586+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:23.587+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Starting task 0.1 in stage 24.0 (TID 97) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:23.588+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.596+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Lost task 0.1 in stage 24.0 (TID 97) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:23.600+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Starting task 0.2 in stage 24.0 (TID 98) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:23.621+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Lost task 0.2 in stage 24.0 (TID 98) on 172.18.0.6, executor 1: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:23.622+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Starting task 0.3 in stage 24.0 (TID 99) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:23.653+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:23.683+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSetManager: Lost task 0.3 in stage 24.0 (TID 99) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:23.684+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 ERROR TaskSetManager: Task 0 in stage 24.0 failed 4 times; aborting job
[2025-02-04T21:46:23.685+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2025-02-04T21:46:23.686+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSchedulerImpl: Cancelling stage 24
[2025-02-04T21:46:23.687+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 99) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:23.688+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:23.689+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:23.692+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:23.697+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:23.701+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:23.705+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:23.706+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:23.709+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:23.714+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:23.717+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:23.718+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:23.719+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:23.719+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:23.720+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:23.721+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:23.722+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:23.722+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:23.723+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:23.724+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:23.724+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:23.725+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:23.726+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:23.726+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:23.727+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:41.parquet does not exist
[2025-02-04T21:46:23.728+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:23.728+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:23.729+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:23.730+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:23.733+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:23.734+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:23.735+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:23.736+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:23.739+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:23.740+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:23.741+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:23.743+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:23.743+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:23.744+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:23.745+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:23.746+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:23.746+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:23.747+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:23.748+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:23.750+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:23.800+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:23.804+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:23.819+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:23.825+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:23.827+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:23.830+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:23.831+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:23.834+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:23.835+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:23.836+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: ResultStage 24 (parquet at <unknown>:0) failed in 0.276 s due to Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 99) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:23.837+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:23.837+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:23.838+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:23.843+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:23.845+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:23.846+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:23.846+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:23.847+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:23.848+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:23.850+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:23.851+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:23.852+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:23.853+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:23.854+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:23.855+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:23.856+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:23.857+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:23.858+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:23.859+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:23.860+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:23.860+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:23.861+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:23.862+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:23.863+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:41.parquet does not exist
[2025-02-04T21:46:23.864+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:23.866+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:23.867+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:23.868+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:23.869+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:23.870+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:23.872+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:23.873+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:23.874+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:23.876+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:23.877+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:23.878+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:23.878+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:23.879+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:23.880+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:23.881+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:23.883+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:23.884+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:23.885+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:23.886+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:23.888+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:23.889+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:23.890+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:23.890+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:23.891+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:23.892+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:23.893+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:23.894+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:23.895+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:23.896+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO DAGScheduler: Job 24 failed: parquet at <unknown>:0, took 0.281953 s
[2025-02-04T21:46:23.960+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:41.parquet. Check logs for details.
[2025-02-04T21:46:23.961+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:33.parquet
[2025-02-04T21:46:23.979+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:24.011+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:24.013+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Got job 25 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:24.015+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Final stage: ResultStage 25 (parquet at <unknown>:0)
[2025-02-04T21:46:24.016+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:24.017+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:24.017+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[51] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:24.021+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:24.029+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:24.038+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:24.039+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_24_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.043+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:24.045+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:24.049+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2025-02-04T21:46:24.053+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 100) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:24.060+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.061+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.092+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.102+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.126+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 WARN TaskSetManager: Lost task 0.0 in stage 25.0 (TID 100) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:24.129+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:24.135+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:24.135+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:24.139+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:24.140+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:24.141+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:24.146+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:24.147+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:24.150+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:24.151+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:24.152+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:24.155+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:24.156+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:24.157+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:24.158+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:24.162+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:24.163+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:24.164+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:24.165+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:24.166+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:24.168+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:24.169+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:24.170+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:24.171+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:33.parquet does not exist
[2025-02-04T21:46:24.172+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:24.173+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:24.175+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:24.176+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:24.177+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:24.179+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:24.180+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:24.182+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:24.184+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:24.186+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:24.187+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:24.190+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:24.192+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:24.193+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:24.197+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:24.198+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:24.199+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:24.200+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:24.200+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:24.201+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:24.201+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:24.203+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:24.205+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:24.207+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:24.209+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:24.210+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:24.211+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:24.212+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:24.213+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Starting task 0.1 in stage 25.0 (TID 101) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:24.213+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.214+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Lost task 0.1 in stage 25.0 (TID 101) on 172.18.0.5, executor 6: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:24.215+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Starting task 0.2 in stage 25.0 (TID 102) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:24.251+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.337+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Lost task 0.2 in stage 25.0 (TID 102) on 172.18.0.5, executor 8: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:24.341+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Starting task 0.3 in stage 25.0 (TID 103) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:24.415+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.482+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Lost task 0.3 in stage 25.0 (TID 103) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:24.483+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 ERROR TaskSetManager: Task 0 in stage 25.0 failed 4 times; aborting job
[2025-02-04T21:46:24.485+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2025-02-04T21:46:24.486+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSchedulerImpl: Cancelling stage 25
[2025-02-04T21:46:24.487+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 25.0 failed 4 times, most recent failure: Lost task 0.3 in stage 25.0 (TID 103) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:24.490+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:24.491+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:24.493+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:24.499+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:24.503+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:24.504+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:24.509+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:24.510+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:24.511+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:24.512+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:24.513+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:24.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:24.515+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:24.517+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:24.518+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:24.518+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:24.519+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:24.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:24.520+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:24.521+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:24.522+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:24.523+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:24.524+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:24.525+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:33.parquet does not exist
[2025-02-04T21:46:24.527+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:24.527+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:24.529+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:24.529+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:24.530+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:24.533+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:24.533+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:24.534+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:24.547+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:24.555+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:24.557+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:24.563+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:24.567+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:24.569+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:24.569+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:24.571+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:24.573+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:24.577+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:24.578+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:24.579+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:24.581+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:24.582+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:24.583+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:24.584+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:24.584+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:24.586+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:24.587+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:24.589+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:24.592+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:24.592+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: ResultStage 25 (parquet at <unknown>:0) failed in 0.471 s due to Job aborted due to stage failure: Task 0 in stage 25.0 failed 4 times, most recent failure: Lost task 0.3 in stage 25.0 (TID 103) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:24.593+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:24.594+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:24.595+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:24.596+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:24.598+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:24.600+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:24.601+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:24.602+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:24.603+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:24.603+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:24.604+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:24.605+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:24.606+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:24.607+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:24.608+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:24.609+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:24.610+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:24.610+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:24.611+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:24.612+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:24.613+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:24.613+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:24.614+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:24.615+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:33.parquet does not exist
[2025-02-04T21:46:24.616+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:24.621+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:24.621+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:24.622+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:24.623+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:24.624+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:24.624+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:24.625+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:24.626+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:24.627+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:24.628+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:24.629+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:24.630+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:24.631+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:24.635+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:24.637+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:24.642+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:24.649+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:24.651+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:24.660+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:24.663+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:24.667+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:24.673+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:24.728+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:24.730+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:24.730+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:24.733+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:24.734+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:24.736+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:24.737+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Job 25 failed: parquet at <unknown>:0, took 0.475607 s
[2025-02-04T21:46:24.738+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:33.parquet. Check logs for details.
[2025-02-04T21:46:24.739+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:23.parquet
[2025-02-04T21:46:24.753+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
[2025-02-04T21:46:24.800+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:24.806+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Got job 26 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:24.809+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Final stage: ResultStage 26 (parquet at <unknown>:0)
[2025-02-04T21:46:24.811+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:24.813+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:24.813+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[53] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:24.817+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:24.826+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:24.829+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:24.832+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:24.837+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:24.842+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-02-04T21:46:24.845+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 104) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:24.847+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_25_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.851+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.853+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.856+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.862+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.891+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:24.934+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 104) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:24.935+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:24.940+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:24.943+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:24.945+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:24.958+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:24.963+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:24.964+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:24.971+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:24.975+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:24.979+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:24.980+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:24.995+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:25.000+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:25.002+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:25.004+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:25.005+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:25.006+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:25.007+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:25.009+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:25.010+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:25.011+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:25.012+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:25.018+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:25.022+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:23.parquet does not exist
[2025-02-04T21:46:25.023+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:25.025+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:25.027+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:25.027+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:25.028+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:25.033+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:25.035+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:25.036+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:25.039+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:25.040+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:25.044+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:25.045+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:25.046+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:25.047+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:25.047+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:25.049+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:25.051+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:25.053+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:25.054+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:25.055+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:25.058+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:25.059+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:25.060+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:25.061+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:25.062+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:25.063+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:25.063+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:25.065+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:25.066+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Starting task 0.1 in stage 26.0 (TID 105) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:25.075+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.076+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Lost task 0.1 in stage 26.0 (TID 105) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:25.078+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:24 INFO TaskSetManager: Starting task 0.2 in stage 26.0 (TID 106) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:25.079+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Lost task 0.2 in stage 26.0 (TID 106) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:25.080+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Starting task 0.3 in stage 26.0 (TID 107) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:25.082+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.6:38973 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.101+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Lost task 0.3 in stage 26.0 (TID 107) on 172.18.0.6, executor 3: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:25.104+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 ERROR TaskSetManager: Task 0 in stage 26.0 failed 4 times; aborting job
[2025-02-04T21:46:25.105+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-02-04T21:46:25.109+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSchedulerImpl: Cancelling stage 26
[2025-02-04T21:46:25.110+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 26.0 failed 4 times, most recent failure: Lost task 0.3 in stage 26.0 (TID 107) (172.18.0.6 executor 3): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:25.138+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:25.146+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:25.152+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:25.165+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:25.178+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:25.181+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:25.188+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:25.190+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:25.197+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:25.198+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:25.199+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:25.200+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:25.201+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:25.202+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:25.203+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:25.204+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:25.206+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:25.209+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:25.209+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:25.210+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:25.211+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:25.212+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:25.213+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:25.215+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:23.parquet does not exist
[2025-02-04T21:46:25.215+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:25.216+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:25.217+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:25.217+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:25.218+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:25.218+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:25.218+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:25.219+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:25.219+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:25.219+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:25.220+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:25.220+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:25.221+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:25.221+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:25.222+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:25.222+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:25.223+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:25.224+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:25.225+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:25.226+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:25.227+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:25.229+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:25.231+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:25.232+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:25.233+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:25.234+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:25.235+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:25.236+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:25.238+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:25.239+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: ResultStage 26 (parquet at <unknown>:0) failed in 0.303 s due to Job aborted due to stage failure: Task 0 in stage 26.0 failed 4 times, most recent failure: Lost task 0.3 in stage 26.0 (TID 107) (172.18.0.6 executor 3): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:25.240+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:25.241+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:25.242+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:25.244+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:25.245+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:25.245+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:25.246+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:25.247+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:25.248+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:25.249+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:25.250+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:25.253+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:25.255+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:25.261+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:25.262+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:25.263+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:25.267+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:25.272+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:25.274+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:25.275+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:25.276+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:25.277+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:25.277+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:25.278+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:23.parquet does not exist
[2025-02-04T21:46:25.279+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:25.280+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:25.281+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:25.282+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:25.283+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:25.283+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:25.284+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:25.285+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:25.286+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:25.287+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:25.287+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:25.288+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:25.289+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:25.291+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:25.292+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:25.292+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:25.293+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:25.293+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:25.294+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:25.295+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:25.295+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:25.298+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:25.299+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:25.300+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:25.305+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:25.306+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:25.307+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:25.308+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:25.309+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:25.310+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Job 26 failed: parquet at <unknown>:0, took 0.308013 s
[2025-02-04T21:46:25.377+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:23.parquet. Check logs for details.
[2025-02-04T21:46:25.378+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 12:00.parquet
[2025-02-04T21:46:25.457+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:25.493+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:25.495+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Got job 27 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:25.496+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Final stage: ResultStage 27 (parquet at <unknown>:0)
[2025-02-04T21:46:25.496+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:25.497+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:25.498+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[55] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:25.505+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:25.514+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:25.517+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:25.521+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:25.523+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Removed broadcast_26_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.526+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.528+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:25.531+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2025-02-04T21:46:25.533+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 108) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:25.537+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.6:38973 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.539+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.575+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.603+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 108) (172.18.0.6 executor 4): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:25.608+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:25.652+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:25.653+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:25.654+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:25.656+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:25.658+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:25.659+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:25.660+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:25.661+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:25.662+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:25.663+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:25.667+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:25.668+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:25.670+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:25.671+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:25.673+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:25.675+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:25.678+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:25.679+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:25.680+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:25.682+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:25.684+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:25.686+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:25.687+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:00.parquet does not exist
[2025-02-04T21:46:25.688+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:25.690+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:25.691+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:25.692+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:25.703+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:25.704+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:25.706+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:25.708+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:25.709+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:25.709+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:25.710+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:25.711+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:25.712+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:25.712+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:25.713+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:25.714+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:25.716+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:25.717+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:25.718+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:25.719+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:25.720+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:25.721+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:25.721+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:25.722+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:25.725+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:25.727+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:25.728+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:25.729+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:25.730+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Starting task 0.1 in stage 27.0 (TID 109) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:25.733+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.734+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Lost task 0.1 in stage 27.0 (TID 109) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:25.735+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Starting task 0.2 in stage 27.0 (TID 110) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:25.749+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.776+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Lost task 0.2 in stage 27.0 (TID 110) on 172.18.0.5, executor 8: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:25.777+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Starting task 0.3 in stage 27.0 (TID 111) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:25.805+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:25.846+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSetManager: Lost task 0.3 in stage 27.0 (TID 111) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:25.847+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 ERROR TaskSetManager: Task 0 in stage 27.0 failed 4 times; aborting job
[2025-02-04T21:46:25.850+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2025-02-04T21:46:25.851+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSchedulerImpl: Cancelling stage 27
[2025-02-04T21:46:25.862+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 27.0 failed 4 times, most recent failure: Lost task 0.3 in stage 27.0 (TID 111) (172.18.0.5 executor 9): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:25.865+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:25.869+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:25.876+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:25.880+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:25.882+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:25.885+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:25.894+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:25.895+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:25.896+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:25.897+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:25.899+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:25.902+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:25.903+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:25.904+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:25.904+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:25.905+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:25.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:25.911+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:25.912+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:25.913+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:25.915+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:25.916+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:25.920+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:25.922+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:00.parquet does not exist
[2025-02-04T21:46:25.923+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:25.927+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:25.928+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:25.929+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:25.930+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:25.931+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:25.933+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:25.935+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:25.938+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:25.942+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:25.943+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:25.945+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:25.946+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:25.947+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:25.948+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:25.949+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:25.956+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:25.958+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:25.959+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:25.959+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:25.961+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:25.962+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:25.964+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:25.966+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:25.967+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:25.968+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:25.968+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:25.969+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:25.970+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:25.972+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: ResultStage 27 (parquet at <unknown>:0) failed in 0.351 s due to Job aborted due to stage failure: Task 0 in stage 27.0 failed 4 times, most recent failure: Lost task 0.3 in stage 27.0 (TID 111) (172.18.0.5 executor 9): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:25.974+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:25.976+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:25.978+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:25.978+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:25.980+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:25.982+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:25.983+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:25.985+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:25.985+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:25.986+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:25.989+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:25.990+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:25.990+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:25.992+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:25.995+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:25.996+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:25.997+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:25.998+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:26.000+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:26.000+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:26.002+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:26.004+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:26.004+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:26.008+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:00.parquet does not exist
[2025-02-04T21:46:26.009+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:26.010+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:26.011+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:26.011+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:26.016+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:26.017+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:26.018+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:26.019+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:26.020+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:26.021+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:26.023+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:26.025+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:26.028+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:26.029+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:26.045+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:26.045+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:26.046+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:26.047+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:26.051+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:26.052+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:26.053+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:26.054+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:26.054+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:26.056+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:26.058+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:26.059+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:26.060+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:26.061+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:26.062+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:26.062+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:25 INFO DAGScheduler: Job 27 failed: parquet at <unknown>:0, took 0.356240 s
[2025-02-04T21:46:26.104+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 12:00.parquet. Check logs for details.
[2025-02-04T21:46:26.105+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:40.parquet
[2025-02-04T21:46:26.135+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.
[2025-02-04T21:46:26.182+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:26.183+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: Got job 28 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:26.185+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: Final stage: ResultStage 28 (parquet at <unknown>:0)
[2025-02-04T21:46:26.187+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:26.188+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:26.191+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[57] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:26.209+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:26.219+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:26.226+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:26.234+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Removed broadcast_27_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.238+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:26.244+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:26.246+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2025-02-04T21:46:26.255+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 112) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:26.256+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.260+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.261+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.265+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.270+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.297+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 112) (172.18.0.6 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:26.298+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:26.300+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:26.301+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:26.302+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:26.302+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:26.303+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:26.304+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:26.305+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:26.305+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:26.306+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:26.309+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:26.311+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:26.312+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:26.313+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:26.316+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:26.317+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:26.318+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:26.320+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:26.321+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:26.323+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:26.325+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:26.325+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:26.326+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:26.327+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:40.parquet does not exist
[2025-02-04T21:46:26.328+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:26.328+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:26.329+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:26.330+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:26.331+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:26.332+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:26.336+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:26.337+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:26.338+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:26.340+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:26.341+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:26.342+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:26.343+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:26.343+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:26.344+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:26.345+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:26.348+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:26.349+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:26.350+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:26.351+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:26.352+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:26.353+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:26.358+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:26.360+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:26.361+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:26.362+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:26.363+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:26.365+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:26.369+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSetManager: Starting task 0.1 in stage 28.0 (TID 113) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:26.370+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.371+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSetManager: Lost task 0.1 in stage 28.0 (TID 113) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:26.373+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSetManager: Starting task 0.2 in stage 28.0 (TID 114) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:26.391+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:26.423+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSetManager: Lost task 0.2 in stage 28.0 (TID 114) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:26.430+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSetManager: Starting task 0.3 in stage 28.0 (TID 115) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:26.469+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSetManager: Lost task 0.3 in stage 28.0 (TID 115) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:26.470+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 ERROR TaskSetManager: Task 0 in stage 28.0 failed 4 times; aborting job
[2025-02-04T21:46:26.471+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2025-02-04T21:46:26.472+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSchedulerImpl: Cancelling stage 28
[2025-02-04T21:46:26.474+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 4 times, most recent failure: Lost task 0.3 in stage 28.0 (TID 115) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:26.475+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:26.475+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:26.476+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:26.477+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:26.477+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:26.478+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:26.479+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:26.483+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:26.489+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:26.493+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:26.496+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:26.498+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:26.499+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:26.501+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:26.501+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:26.503+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:26.504+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:26.505+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:26.509+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:26.510+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:26.513+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:26.516+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:26.518+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:26.526+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:40.parquet does not exist
[2025-02-04T21:46:26.531+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:26.532+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:26.532+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:26.534+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:26.534+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:26.536+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:26.537+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:26.539+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:26.539+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:26.542+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:26.543+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:26.544+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:26.546+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:26.546+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:26.550+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:26.552+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:26.552+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:26.553+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:26.554+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:26.557+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:26.558+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:26.559+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:26.561+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:26.563+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:26.564+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:26.566+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:26.567+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:26.570+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:26.571+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:26.572+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: ResultStage 28 (parquet at <unknown>:0) failed in 0.286 s due to Job aborted due to stage failure: Task 0 in stage 28.0 failed 4 times, most recent failure: Lost task 0.3 in stage 28.0 (TID 115) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:26.573+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:26.576+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:26.577+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:26.578+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:26.580+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:26.581+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:26.582+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:26.583+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:26.584+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:26.585+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:26.586+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:26.587+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:26.588+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:26.589+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:26.590+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:26.591+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:26.592+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:26.593+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:26.594+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:26.595+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:26.595+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:26.596+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:26.597+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:26.598+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:40.parquet does not exist
[2025-02-04T21:46:26.598+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:26.599+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:26.600+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:26.601+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:26.602+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:26.603+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:26.603+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:26.604+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:26.605+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:26.605+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:26.606+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:26.607+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:26.608+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:26.609+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:26.610+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:26.610+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:26.611+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:26.612+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:26.613+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:26.614+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:26.615+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:26.616+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:26.617+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:26.617+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:26.618+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:26.720+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:26.728+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:26.729+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:26.736+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:26.737+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO DAGScheduler: Job 28 failed: parquet at <unknown>:0, took 0.291070 s
[2025-02-04T21:46:26.907+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:40.parquet. Check logs for details.
[2025-02-04T21:46:26.908+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 06:26.parquet
[2025-02-04T21:46:26.936+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:27.006+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:27.007+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Got job 29 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:27.011+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Final stage: ResultStage 29 (parquet at <unknown>:0)
[2025-02-04T21:46:27.015+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:27.015+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:27.019+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[59] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:27.022+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:27.042+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:27.049+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:27.051+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:27.055+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:27.069+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2025-02-04T21:46:27.072+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 116) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:27.079+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.082+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.088+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.090+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.101+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.129+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 WARN TaskSetManager: Lost task 0.0 in stage 29.0 (TID 116) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:27.132+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:27.134+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:27.135+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:27.136+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:27.136+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:27.139+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:27.143+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:27.148+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:27.152+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:27.158+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:27.162+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:27.166+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:27.169+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:27.171+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:27.184+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:27.186+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:27.192+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:27.195+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:27.198+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:27.201+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:27.203+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:27.206+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:27.208+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:27.211+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:26.parquet does not exist
[2025-02-04T21:46:27.218+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:27.220+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:27.225+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:27.228+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:27.229+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:27.232+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:27.236+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:27.237+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:27.238+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:27.239+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:27.240+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:27.241+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:27.243+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:27.244+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:27.248+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:27.255+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:27.258+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:27.260+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:27.261+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:27.263+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:27.264+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:27.266+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:27.267+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:27.268+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:27.268+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:27.269+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:27.270+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:27.271+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:27.273+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.1 in stage 29.0 (TID 117) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:27.274+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.6:35135 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.275+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Lost task 0.1 in stage 29.0 (TID 117) on 172.18.0.6, executor 0: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:27.276+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.2 in stage 29.0 (TID 118) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:27.283+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.309+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Lost task 0.2 in stage 29.0 (TID 118) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:27.313+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.3 in stage 29.0 (TID 119) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:27.347+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.6:38973 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.373+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Lost task 0.3 in stage 29.0 (TID 119) on 172.18.0.6, executor 3: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:27.374+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 ERROR TaskSetManager: Task 0 in stage 29.0 failed 4 times; aborting job
[2025-02-04T21:46:27.378+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2025-02-04T21:46:27.379+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSchedulerImpl: Cancelling stage 29
[2025-02-04T21:46:27.381+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 29.0 failed 4 times, most recent failure: Lost task 0.3 in stage 29.0 (TID 119) (172.18.0.6 executor 3): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:27.382+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:27.384+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:27.385+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:27.386+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:27.390+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:27.391+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:27.395+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:27.399+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:27.400+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:27.400+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:27.401+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:27.402+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:27.403+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:27.403+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:27.404+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:27.405+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:27.406+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:27.407+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:27.408+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:27.409+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:27.410+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:27.410+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:27.411+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:27.413+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:26.parquet does not exist
[2025-02-04T21:46:27.414+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:27.415+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:27.415+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:27.417+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:27.418+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:27.418+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:27.420+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:27.421+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:27.422+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:27.423+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:27.424+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:27.424+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:27.425+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:27.425+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:27.426+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:27.426+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:27.427+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:27.428+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:27.430+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:27.433+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:27.434+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:27.434+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:27.435+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:27.436+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:27.436+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:27.436+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:27.437+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:27.438+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:27.439+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:27.440+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: ResultStage 29 (parquet at <unknown>:0) failed in 0.365 s due to Job aborted due to stage failure: Task 0 in stage 29.0 failed 4 times, most recent failure: Lost task 0.3 in stage 29.0 (TID 119) (172.18.0.6 executor 3): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:27.440+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:27.441+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:27.441+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:27.442+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:27.442+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:27.443+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:27.444+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:27.445+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:27.446+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:27.447+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:27.447+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:27.448+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:27.448+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:27.449+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:27.449+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:27.450+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:27.450+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:27.451+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:27.451+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:27.452+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:27.452+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:27.453+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:27.453+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:27.454+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 06:26.parquet does not exist
[2025-02-04T21:46:27.455+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:27.456+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:27.456+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:27.457+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:27.458+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:27.458+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:27.459+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:27.460+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:27.460+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:27.461+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:27.461+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:27.462+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:27.463+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:27.464+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:27.464+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:27.465+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:27.466+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:27.467+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:27.467+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:27.468+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:27.468+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:27.469+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:27.469+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:27.470+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:27.471+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:27.471+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:27.472+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:27.473+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:27.473+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:27.474+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Job 29 failed: parquet at <unknown>:0, took 0.368277 s
[2025-02-04T21:46:27.594+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 06:26.parquet. Check logs for details.
[2025-02-04T21:46:27.595+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 11:49.parquet
[2025-02-04T21:46:27.609+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:27.645+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:27.647+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Got job 30 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:27.648+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Final stage: ResultStage 30 (parquet at <unknown>:0)
[2025-02-04T21:46:27.649+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:27.650+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:27.650+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[61] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:27.657+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:27.670+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:27.671+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:27.673+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.675+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:27.676+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:27.679+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2025-02-04T21:46:27.686+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 120) (172.18.0.5, executor 6, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:27.691+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.702+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.6:35135 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.706+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.6:38973 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.709+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.723+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.5:34691 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.748+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 120) (172.18.0.5 executor 6): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:27.749+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:27.750+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:27.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:27.751+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:27.752+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:27.753+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:27.754+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:27.755+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:27.756+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:27.757+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:27.758+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:27.759+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:27.760+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:27.760+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:27.761+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:27.762+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:27.762+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:27.763+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:27.764+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:27.765+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:27.765+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:27.766+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:27.766+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:27.767+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:49.parquet does not exist
[2025-02-04T21:46:27.767+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:27.769+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:27.770+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:27.771+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:27.771+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:27.772+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:27.772+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:27.773+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:27.774+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:27.775+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:27.775+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:27.776+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:27.777+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:27.778+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:27.779+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:27.780+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:27.781+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:27.782+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:27.783+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:27.783+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:27.784+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:27.789+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:27.790+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:27.791+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:27.792+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:27.793+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:27.794+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:27.794+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:27.795+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.1 in stage 30.0 (TID 121) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:27.797+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.6:38973 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.824+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Lost task 0.1 in stage 30.0 (TID 121) on 172.18.0.6, executor 3: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:27.826+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.2 in stage 30.0 (TID 122) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:27.920+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:27.964+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Lost task 0.2 in stage 30.0 (TID 122) on 172.18.0.6, executor 2: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:27.979+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:27 INFO TaskSetManager: Starting task 0.3 in stage 30.0 (TID 123) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:28.038+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.6:46693 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.066+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSetManager: Lost task 0.3 in stage 30.0 (TID 123) on 172.18.0.6, executor 1: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:28.077+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 ERROR TaskSetManager: Task 0 in stage 30.0 failed 4 times; aborting job
[2025-02-04T21:46:28.077+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2025-02-04T21:46:28.078+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSchedulerImpl: Cancelling stage 30
[2025-02-04T21:46:28.080+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 123) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:28.081+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:28.083+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:28.084+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:28.086+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:28.092+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:28.093+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:28.094+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:28.095+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:28.097+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:28.098+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:28.102+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:28.103+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:28.104+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:28.106+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:28.107+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:28.108+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:28.109+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:28.111+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:28.115+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:28.117+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:28.120+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:28.122+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:28.125+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:28.126+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:49.parquet does not exist
[2025-02-04T21:46:28.127+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:28.133+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:28.134+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:28.135+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:28.136+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:28.138+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:28.144+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:28.145+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:28.146+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:28.150+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:28.153+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:28.158+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:28.163+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:28.166+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:28.168+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:28.171+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:28.173+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:28.177+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:28.178+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:28.183+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:28.186+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:28.188+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:28.190+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:28.192+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:28.197+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:28.198+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:28.199+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:28.202+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:28.206+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:28.207+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: ResultStage 30 (parquet at <unknown>:0) failed in 0.419 s due to Job aborted due to stage failure: Task 0 in stage 30.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30.0 (TID 123) (172.18.0.6 executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:28.208+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:28.209+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:28.210+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:28.211+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:28.212+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:28.213+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:28.214+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:28.216+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:28.216+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:28.217+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:28.219+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:28.220+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:28.221+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:28.222+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:28.223+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:28.225+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:28.226+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:28.228+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:28.234+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:28.235+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:28.238+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:28.239+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:28.240+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:28.241+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 11:49.parquet does not exist
[2025-02-04T21:46:28.245+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:28.245+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:28.246+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:28.247+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:28.248+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:28.250+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:28.251+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:28.252+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:28.253+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:28.253+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:28.255+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:28.256+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:28.257+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:28.259+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:28.261+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:28.262+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:28.264+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:28.265+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:28.267+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:28.268+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:28.269+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:28.270+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:28.271+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:28.273+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:28.274+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:28.277+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:28.279+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:28.279+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:28.281+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:28.282+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: Job 30 failed: parquet at <unknown>:0, took 0.424118 s
[2025-02-04T21:46:28.419+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 11:49.parquet. Check logs for details.
[2025-02-04T21:46:28.421+0000] {spark_submit.py:641} INFO - Processing: 2025-01-22 12:08.parquet
[2025-02-04T21:46:28.467+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2025-02-04T21:46:28.556+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:28.559+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: Got job 31 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:28.564+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: Final stage: ResultStage 31 (parquet at <unknown>:0)
[2025-02-04T21:46:28.565+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:28.566+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:28.567+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[63] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:28.573+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:28.583+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:28.628+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:28.628+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:28.629+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:28.633+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2025-02-04T21:46:28.634+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.638+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.6:46693 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.640+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 124) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:28.641+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.5:34691 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.641+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.642+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.6:38973 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.656+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.688+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 WARN TaskSetManager: Lost task 0.0 in stage 31.0 (TID 124) (172.18.0.5 executor 7): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:28.689+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:28.691+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:28.694+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:28.695+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:28.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:28.697+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:28.700+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:28.701+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:28.702+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:28.703+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:28.704+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:28.705+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:28.706+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:28.708+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:28.709+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:28.710+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:28.711+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:28.713+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:28.714+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:28.808+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:28.819+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:28.845+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:28.847+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:28.857+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:08.parquet does not exist
[2025-02-04T21:46:28.878+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:28.880+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:28.881+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:28.885+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:28.886+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:28.888+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:28.889+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:28.892+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:28.894+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:28.900+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:28.901+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:28.902+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:28.902+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:28.903+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:28.905+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:28.914+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:28.926+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:28.927+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:28.928+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:28.929+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:28.934+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:28.935+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:28.940+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:28.941+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:28.943+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:28.944+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:28.946+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:28.947+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:28.949+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSetManager: Starting task 0.1 in stage 31.0 (TID 125) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:28.950+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.951+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSetManager: Lost task 0.1 in stage 31.0 (TID 125) on 172.18.0.5, executor 8: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:28.954+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSetManager: Starting task 0.2 in stage 31.0 (TID 126) (172.18.0.6, executor 3, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:28.957+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.6:38973 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:28.972+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSetManager: Lost task 0.2 in stage 31.0 (TID 126) on 172.18.0.6, executor 3: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:28.974+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:28 INFO TaskSetManager: Starting task 0.3 in stage 31.0 (TID 127) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:29.100+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.151+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Lost task 0.3 in stage 31.0 (TID 127) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:29.161+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 ERROR TaskSetManager: Task 0 in stage 31.0 failed 4 times; aborting job
[2025-02-04T21:46:29.163+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2025-02-04T21:46:29.167+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSchedulerImpl: Cancelling stage 31
[2025-02-04T21:46:29.168+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 31.0 failed 4 times, most recent failure: Lost task 0.3 in stage 31.0 (TID 127) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:29.168+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:29.169+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:29.171+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:29.173+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:29.174+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:29.177+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:29.178+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:29.179+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:29.180+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:29.181+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:29.182+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:29.184+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:29.185+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:29.186+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:29.187+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:29.188+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:29.189+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:29.189+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:29.190+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:29.191+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:29.192+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:29.193+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:29.195+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:29.196+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:08.parquet does not exist
[2025-02-04T21:46:29.197+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:29.198+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:29.199+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:29.200+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:29.200+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:29.202+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:29.202+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:29.203+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:29.204+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:29.205+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:29.206+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:29.206+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:29.207+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:29.208+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:29.208+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:29.209+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:29.210+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:29.211+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:29.212+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:29.213+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:29.214+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:29.215+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:29.216+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:29.217+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:29.218+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:29.219+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:29.220+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:29.221+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:29.222+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:29.223+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: ResultStage 31 (parquet at <unknown>:0) failed in 0.596 s due to Job aborted due to stage failure: Task 0 in stage 31.0 failed 4 times, most recent failure: Lost task 0.3 in stage 31.0 (TID 127) (172.18.0.6 executor 5): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:29.223+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:29.224+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:29.225+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:29.226+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:29.227+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:29.228+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:29.229+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:29.230+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:29.230+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:29.231+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:29.231+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:29.232+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:29.233+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:29.233+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:29.234+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:29.235+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:29.236+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:29.236+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:29.238+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:29.239+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:29.240+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:29.240+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:29.241+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:29.242+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-22/2025-01-22 12:08.parquet does not exist
[2025-02-04T21:46:29.242+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:29.243+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:29.243+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:29.244+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:29.244+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:29.245+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:29.246+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:29.246+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:29.247+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:29.247+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:29.248+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:29.248+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:29.249+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:29.249+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:29.249+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:29.250+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:29.251+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:29.251+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:29.252+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:29.253+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:29.254+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:29.255+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:29.256+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:29.257+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:29.258+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:29.259+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:29.259+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:29.260+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:29.261+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:29.262+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Job 31 failed: parquet at <unknown>:0, took 0.601735 s
[2025-02-04T21:46:29.401+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-22 12:08.parquet. Check logs for details.
[2025-02-04T21:46:29.403+0000] {spark_submit.py:641} INFO - Processing: 2025-01-15 18:40.parquet
[2025-02-04T21:46:29.436+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2025-02-04T21:46:29.489+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:29.494+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Got job 32 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:29.496+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Final stage: ResultStage 32 (parquet at <unknown>:0)
[2025-02-04T21:46:29.497+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:29.498+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:29.501+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:29.504+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:29.514+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:29.515+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:29.523+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Removed broadcast_31_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.526+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:29.531+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:29.535+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2025-02-04T21:46:29.538+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.6:38973 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.539+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 128) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:29.548+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.550+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.552+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.6:34233 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.598+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.5:43159 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.631+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 WARN TaskSetManager: Lost task 0.0 in stage 32.0 (TID 128) (172.18.0.5 executor 7): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:29.635+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:29.636+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:29.640+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:29.642+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:29.643+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:29.644+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:29.645+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:29.647+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:29.648+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:29.649+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:29.649+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:29.650+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:29.651+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:29.652+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:29.653+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:29.653+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:29.654+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:29.656+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:29.657+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:29.659+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:29.660+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:29.661+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:29.668+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:29.670+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-15/2025-01-15 18:40.parquet does not exist
[2025-02-04T21:46:29.673+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:29.676+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:29.680+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:29.682+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:29.685+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:29.686+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:29.687+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:29.688+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:29.689+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:29.694+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:29.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:29.697+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:29.699+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:29.701+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:29.703+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:29.704+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:29.705+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:29.706+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:29.707+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:29.708+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:29.709+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:29.714+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:29.716+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:29.717+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:29.718+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:29.718+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:29.719+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:29.721+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:29.723+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Starting task 0.1 in stage 32.0 (TID 129) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:29.725+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.725+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Lost task 0.1 in stage 32.0 (TID 129) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:29.727+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Starting task 0.2 in stage 32.0 (TID 130) (172.18.0.5, executor 7, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:29.743+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Lost task 0.2 in stage 32.0 (TID 130) on 172.18.0.5, executor 7: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:29.744+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Starting task 0.3 in stage 32.0 (TID 131) (172.18.0.5, executor 10, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:29.773+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.5:37863 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:29.851+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSetManager: Lost task 0.3 in stage 32.0 (TID 131) on 172.18.0.5, executor 10: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:29.852+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 ERROR TaskSetManager: Task 0 in stage 32.0 failed 4 times; aborting job
[2025-02-04T21:46:29.853+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2025-02-04T21:46:29.858+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSchedulerImpl: Cancelling stage 32
[2025-02-04T21:46:29.868+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 32.0 failed 4 times, most recent failure: Lost task 0.3 in stage 32.0 (TID 131) (172.18.0.5 executor 10): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:29.870+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:29.873+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:29.874+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:29.875+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:29.876+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:29.877+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:29.878+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:29.881+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:29.885+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:29.891+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:29.898+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:29.899+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:29.901+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:29.902+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:29.903+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:29.906+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:29.907+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:29.908+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:29.909+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:29.910+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:29.911+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:29.912+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:29.914+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:29.915+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-15/2025-01-15 18:40.parquet does not exist
[2025-02-04T21:46:29.917+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:29.919+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:29.920+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:29.922+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:29.924+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:29.925+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:29.925+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:29.926+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:29.927+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:29.928+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:29.929+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:29.930+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:29.932+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:29.933+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:29.934+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:29.936+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:29.937+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:29.938+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:29.940+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:29.940+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:29.941+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:29.942+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:29.953+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:29.956+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:29.958+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:29.960+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:29.964+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:29.965+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:29.966+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:29.967+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: ResultStage 32 (parquet at <unknown>:0) failed in 0.359 s due to Job aborted due to stage failure: Task 0 in stage 32.0 failed 4 times, most recent failure: Lost task 0.3 in stage 32.0 (TID 131) (172.18.0.5 executor 10): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:29.968+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:29.970+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:29.971+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:29.972+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:29.973+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:29.974+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:29.975+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:29.976+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:29.977+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:29.978+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:29.979+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:29.980+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:29.981+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:29.982+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:29.983+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:29.984+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:29.984+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:29.985+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:29.986+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:29.987+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:29.988+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:29.990+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:29.990+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:29.991+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-15/2025-01-15 18:40.parquet does not exist
[2025-02-04T21:46:29.992+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:29.993+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:29.993+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:29.994+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:29.995+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:29.996+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:29.997+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:29.998+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:29.999+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:30.000+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:30.000+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:30.001+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:30.002+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:30.002+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:30.018+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:30.019+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:30.022+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:30.024+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:30.026+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:30.028+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:30.032+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:30.035+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:30.036+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:30.037+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:30.038+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:30.040+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:30.041+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:30.042+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:30.043+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:30.043+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:29 INFO DAGScheduler: Job 32 failed: parquet at <unknown>:0, took 0.363921 s
[2025-02-04T21:46:30.139+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-15 18:40.parquet. Check logs for details.
[2025-02-04T21:46:30.141+0000] {spark_submit.py:641} INFO - Processing: 2025-01-15 22:21.parquet
[2025-02-04T21:46:30.164+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:30.203+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:30.205+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Got job 33 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:30.206+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Final stage: ResultStage 33 (parquet at <unknown>:0)
[2025-02-04T21:46:30.207+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:30.208+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:30.209+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[67] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:30.216+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:30.225+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:30.229+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:30.232+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:30.234+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:30.236+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2025-02-04T21:46:30.244+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 132) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:30.255+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.5:42729 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.260+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.5:37863 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.264+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.5:43159 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.265+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_32_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.298+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.6:37897 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.324+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 132) (172.18.0.6 executor 2): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:30.326+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:30.330+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:30.332+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:30.333+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:30.334+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:30.335+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:30.336+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:30.338+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:30.340+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:30.342+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:30.343+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:30.344+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:30.347+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:30.348+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:30.350+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:30.350+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:30.351+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:30.352+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:30.353+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:30.354+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:30.355+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:30.356+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:30.356+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:30.357+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-15/2025-01-15 22:21.parquet does not exist
[2025-02-04T21:46:30.358+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:30.359+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:30.359+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:30.360+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:30.360+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:30.361+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:30.362+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:30.363+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:30.364+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:30.364+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:30.365+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:30.365+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:30.365+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:30.366+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:30.368+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:30.369+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:30.369+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:30.371+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:30.372+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:30.374+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:30.374+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:30.375+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:30.380+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:30.381+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:30.382+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:30.383+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:30.384+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:30.385+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:30.386+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Starting task 0.1 in stage 33.0 (TID 133) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:30.388+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.389+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Lost task 0.1 in stage 33.0 (TID 133) on 172.18.0.5, executor 8: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:30.391+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Starting task 0.2 in stage 33.0 (TID 134) (172.18.0.6, executor 4, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:30.407+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.6:40911 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.432+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Lost task 0.2 in stage 33.0 (TID 134) on 172.18.0.6, executor 4: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:30.433+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Starting task 0.3 in stage 33.0 (TID 135) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:30.466+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.501+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Lost task 0.3 in stage 33.0 (TID 135) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:30.502+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 ERROR TaskSetManager: Task 0 in stage 33.0 failed 4 times; aborting job
[2025-02-04T21:46:30.503+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2025-02-04T21:46:30.504+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSchedulerImpl: Cancelling stage 33
[2025-02-04T21:46:30.505+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 33.0 failed 4 times, most recent failure: Lost task 0.3 in stage 33.0 (TID 135) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:30.506+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:30.506+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:30.507+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:30.508+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:30.509+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:30.510+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:30.510+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:30.511+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:30.513+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:30.514+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:30.515+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:30.516+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:30.517+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:30.518+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:30.519+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:30.521+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:30.522+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:30.523+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:30.524+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:30.525+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:30.525+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:30.526+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:30.527+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:30.528+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-15/2025-01-15 22:21.parquet does not exist
[2025-02-04T21:46:30.530+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:30.531+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:30.532+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:30.533+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:30.539+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:30.541+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:30.543+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:30.544+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:30.546+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:30.547+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:30.548+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:30.549+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:30.550+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:30.550+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:30.551+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:30.552+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:30.555+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:30.556+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:30.557+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:30.557+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:30.559+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:30.565+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:30.566+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:30.567+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:30.571+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:30.574+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:30.575+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:30.578+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:30.581+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:30.582+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: ResultStage 33 (parquet at <unknown>:0) failed in 0.294 s due to Job aborted due to stage failure: Task 0 in stage 33.0 failed 4 times, most recent failure: Lost task 0.3 in stage 33.0 (TID 135) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:30.582+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:30.583+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:30.584+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:30.585+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:30.588+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:30.592+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:30.646+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:30.648+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:30.650+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:30.651+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:30.656+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:30.658+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:30.659+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:30.660+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:30.661+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:30.662+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:30.663+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:30.664+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:30.665+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:30.666+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:30.667+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:30.668+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:30.669+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:30.670+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-15/2025-01-15 22:21.parquet does not exist
[2025-02-04T21:46:30.675+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:30.676+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:30.677+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:30.686+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:30.687+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:30.688+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:30.689+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:30.690+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:30.690+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:30.691+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:30.692+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:30.694+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:30.696+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:30.700+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:30.701+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:30.702+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:30.705+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:30.709+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:30.710+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:30.712+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:30.713+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:30.715+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:30.718+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:30.720+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:30.721+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:30.727+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:30.730+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:30.736+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:30.738+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:30.739+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Job 33 failed: parquet at <unknown>:0, took 0.302891 s
[2025-02-04T21:46:30.816+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-15 22:21.parquet. Check logs for details.
[2025-02-04T21:46:30.817+0000] {spark_submit.py:641} INFO - Processing: 2025-01-23 04:36.parquet
[2025-02-04T21:46:30.833+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2025-02-04T21:46:30.865+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO SparkContext: Starting job: parquet at <unknown>:0
[2025-02-04T21:46:30.866+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Got job 34 (parquet at <unknown>:0) with 1 output partitions
[2025-02-04T21:46:30.866+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Final stage: ResultStage 34 (parquet at <unknown>:0)
[2025-02-04T21:46:30.867+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Parents of final stage: List()
[2025-02-04T21:46:30.868+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Missing parents: List()
[2025-02-04T21:46:30.869+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[69] at parquet at <unknown>:0), which has no missing parents
[2025-02-04T21:46:30.875+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 103.4 KiB, free 434.2 MiB)
[2025-02-04T21:46:30.888+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 434.1 MiB)
[2025-02-04T21:46:30.893+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on a5e0fb099962:40667 (size: 37.3 KiB, free: 434.3 MiB)
[2025-02-04T21:46:30.902+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_33_piece0 on a5e0fb099962:40667 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.915+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2025-02-04T21:46:30.922+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[69] at parquet at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-02-04T21:46:30.924+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2025-02-04T21:46:30.927+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 136) (172.18.0.5, executor 8, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:30.928+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.5:45661 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.929+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.5:33577 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.931+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.6:40911 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.932+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.6:37897 in memory (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:30.970+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:30 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.5:45661 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:31.012+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 WARN TaskSetManager: Lost task 0.0 in stage 34.0 (TID 136) (172.18.0.5 executor 8): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:31.013+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:31.015+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:31.016+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:31.017+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:31.018+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:31.021+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:31.022+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:31.024+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:31.025+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:31.027+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:31.030+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:31.031+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:31.035+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:31.037+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:31.060+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:31.079+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:31.080+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:31.101+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:31.121+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:31.124+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:31.126+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:31.127+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:31.129+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:31.130+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-23/2025-01-23 04:36.parquet does not exist
[2025-02-04T21:46:31.132+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:31.134+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:31.137+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:31.138+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:31.140+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:31.140+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:31.141+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:31.143+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:31.144+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:31.145+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:31.146+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:31.147+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:31.148+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:31.148+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:31.149+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:31.150+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:31.151+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:31.152+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:31.152+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:31.154+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:31.155+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:31.157+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:31.158+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:31.158+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:31.159+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:31.160+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:31.160+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:31.162+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:31.163+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSetManager: Starting task 0.1 in stage 34.0 (TID 137) (172.18.0.6, executor 5, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:31.165+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.6:34233 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:31.166+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSetManager: Lost task 0.1 in stage 34.0 (TID 137) on 172.18.0.6, executor 5: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 1]
[2025-02-04T21:46:31.167+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSetManager: Starting task 0.2 in stage 34.0 (TID 138) (172.18.0.5, executor 9, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:31.169+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.5:42729 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:31.170+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSetManager: Lost task 0.2 in stage 34.0 (TID 138) on 172.18.0.5, executor 9: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 2]
[2025-02-04T21:46:31.171+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSetManager: Starting task 0.3 in stage 34.0 (TID 139) (172.18.0.5, executor 11, partition 0, PROCESS_LOCAL, 9160 bytes)
[2025-02-04T21:46:31.172+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.5:33577 (size: 37.3 KiB, free: 434.4 MiB)
[2025-02-04T21:46:31.190+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSetManager: Lost task 0.3 in stage 34.0 (TID 139) on 172.18.0.5, executor 11: org.apache.spark.SparkException (Exception thrown in awaitResult: ) [duplicate 3]
[2025-02-04T21:46:31.191+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 ERROR TaskSetManager: Task 0 in stage 34.0 failed 4 times; aborting job
[2025-02-04T21:46:31.193+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2025-02-04T21:46:31.194+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSchedulerImpl: Cancelling stage 34
[2025-02-04T21:46:31.195+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 139) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:31.196+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:31.198+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:31.200+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:31.201+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:31.201+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:31.202+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:31.202+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:31.203+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:31.204+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:31.205+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:31.205+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:31.206+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:31.207+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:31.208+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:31.208+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:31.209+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:31.209+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:31.210+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:31.210+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:31.211+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:31.212+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:31.213+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:31.213+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:31.214+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-23/2025-01-23 04:36.parquet does not exist
[2025-02-04T21:46:31.214+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:31.215+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:31.216+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:31.216+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:31.217+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:31.217+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:31.218+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:31.219+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:31.220+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:31.221+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:31.222+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:31.223+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:31.223+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:31.224+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:31.224+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:31.225+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:31.225+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:31.226+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:31.226+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:31.227+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:31.228+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:31.231+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:31.232+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:31.233+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:31.234+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:31.235+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:31.236+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:31.237+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:31.238+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:31.239+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO DAGScheduler: ResultStage 34 (parquet at <unknown>:0) failed in 0.324 s due to Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 139) (172.18.0.5 executor 11): org.apache.spark.SparkException: Exception thrown in awaitResult:
[2025-02-04T21:46:31.242+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2025-02-04T21:46:31.243+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2025-02-04T21:46:31.244+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
[2025-02-04T21:46:31.245+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)
[2025-02-04T21:46:31.247+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)
[2025-02-04T21:46:31.247+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)
[2025-02-04T21:46:31.248+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
[2025-02-04T21:46:31.249+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)
[2025-02-04T21:46:31.250+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)
[2025-02-04T21:46:31.251+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-02-04T21:46:31.252+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-02-04T21:46:31.253+0000] {spark_submit.py:641} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-02-04T21:46:31.255+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-02-04T21:46:31.256+0000] {spark_submit.py:641} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-02-04T21:46:31.257+0000] {spark_submit.py:641} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-02-04T21:46:31.258+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-02-04T21:46:31.259+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-02-04T21:46:31.262+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-02-04T21:46:31.264+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-02-04T21:46:31.265+0000] {spark_submit.py:641} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-02-04T21:46:31.266+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-02-04T21:46:31.267+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-02-04T21:46:31.267+0000] {spark_submit.py:641} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-04T21:46:31.268+0000] {spark_submit.py:641} INFO - Caused by: java.io.FileNotFoundException: File file:/opt/airflow/data/raw/2025-01-23/2025-01-23 04:36.parquet does not exist
[2025-02-04T21:46:31.269+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
[2025-02-04T21:46:31.269+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
[2025-02-04T21:46:31.270+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
[2025-02-04T21:46:31.271+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)
[2025-02-04T21:46:31.272+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)
[2025-02-04T21:46:31.273+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)
[2025-02-04T21:46:31.274+0000] {spark_submit.py:641} INFO - at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)
[2025-02-04T21:46:31.274+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69)
[2025-02-04T21:46:31.275+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:796)
[2025-02-04T21:46:31.276+0000] {spark_submit.py:641} INFO - at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)
[2025-02-04T21:46:31.276+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)
[2025-02-04T21:46:31.277+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)
[2025-02-04T21:46:31.278+0000] {spark_submit.py:641} INFO - at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)
[2025-02-04T21:46:31.279+0000] {spark_submit.py:641} INFO - at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
[2025-02-04T21:46:31.280+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
[2025-02-04T21:46:31.281+0000] {spark_submit.py:641} INFO - at scala.util.Success.$anonfun$map$1(Try.scala:255)
[2025-02-04T21:46:31.282+0000] {spark_submit.py:641} INFO - at scala.util.Success.map(Try.scala:213)
[2025-02-04T21:46:31.283+0000] {spark_submit.py:641} INFO - at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
[2025-02-04T21:46:31.284+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
[2025-02-04T21:46:31.285+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2025-02-04T21:46:31.286+0000] {spark_submit.py:641} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2025-02-04T21:46:31.289+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
[2025-02-04T21:46:31.290+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
[2025-02-04T21:46:31.291+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
[2025-02-04T21:46:31.291+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
[2025-02-04T21:46:31.292+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
[2025-02-04T21:46:31.292+0000] {spark_submit.py:641} INFO - at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[2025-02-04T21:46:31.293+0000] {spark_submit.py:641} INFO - 
[2025-02-04T21:46:31.294+0000] {spark_submit.py:641} INFO - Driver stacktrace:
[2025-02-04T21:46:31.296+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO DAGScheduler: Job 34 failed: parquet at <unknown>:0, took 0.327461 s
[2025-02-04T21:46:31.535+0000] {spark_submit.py:641} INFO - An error occurred processing 2025-01-23 04:36.parquet. Check logs for details.
[2025-02-04T21:46:31.539+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-02-04T21:46:31.583+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO SparkUI: Stopped Spark web UI at http://a5e0fb099962:4040
[2025-02-04T21:46:31.599+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-02-04T21:46:31.601+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-02-04T21:46:31.718+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-02-04T21:46:31.866+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO MemoryStore: MemoryStore cleared
[2025-02-04T21:46:31.885+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO BlockManager: BlockManager stopped
[2025-02-04T21:46:31.889+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-02-04T21:46:31.932+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-02-04T21:46:32.066+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:32 INFO SparkContext: Successfully stopped SparkContext
[2025-02-04T21:46:32.410+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:32 INFO ShutdownHookManager: Shutdown hook called
[2025-02-04T21:46:32.412+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-68fbff48-59a8-4bdf-a0a6-a968b9586a8e
[2025-02-04T21:46:32.427+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-71163641-08d4-4bc0-981c-b95da9b03065
[2025-02-04T21:46:32.436+0000] {spark_submit.py:641} INFO - 25/02/04 21:46:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-68fbff48-59a8-4bdf-a0a6-a968b9586a8e/pyspark-a7d902f8-76c8-427d-9f50-41d5a5e41c8c
[2025-02-04T21:46:32.561+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-02-04T21:46:32.562+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=Temp_elt_postgres, task_id=transform_weather_data, run_id=manual__2025-02-04T21:44:58.039352+00:00, execution_date=20250204T214458, start_date=20250204T214516, end_date=20250204T214632
[2025-02-04T21:46:32.740+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-04T21:46:32.949+0000] {taskinstance.py:3895} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-04T21:46:32.965+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
